<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>helper.regrassion API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>helper.regrassion</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt

from tabulate import tabulate
from pandas import DataFrame, Series, concat

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.stattools import durbin_watson
from statsmodels.stats.api import het_breuschpagan
from sklearn.model_selection import GridSearchCV

from scipy.stats import t, f
from helper.util import my_pretty_table, my_trend, my_train_test_split
from helper.plot import my_residplot, my_qqplot, my_learing_curve

def my_auto_linear_regrassion(df:DataFrame, yname:str, cv:int=5, learning_curve: bool = True, degree : int = 1, plot: bool = True, report=True, resid_test=False, figsize=(10, 4), dpi=150, sort: str = None,order: str = None,p_value_num:float=0.05) -&gt; LinearRegression:
    &#34;&#34;&#34;ì„ í˜•íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        df (DataFrame) : íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•  ë°ì´í„°í”„ë ˆì„.
        yname (str) : ì¢…ì†ë³€ìˆ˜
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 4).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 150.
        order (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        p_value_num (float, optional) : íšŒê·€ëª¨í˜•ì˜ ìœ ì˜í™•ë¥ . Drfaults to 0.05
    Returns:
        LinearRegression: íšŒê·€ë¶„ì„ ëª¨ë¸
    &#34;&#34;&#34;

    x_train, x_test, y_train, y_test = my_train_test_split(df, yname, test_size=0.2)
    
    xnames = x_train.columns
    yname = y_train.name
    size = len(xnames)

    # ë¶„ì„ëª¨ë¸ ìƒì„±
    model = LinearRegression(n_jobs=-1) # n_jobs : ì‚¬ìš©í•˜ëŠ” cpu ì½”ì–´ì˜ ê°œìˆ˜ // -1ì€ ìµœëŒ€ì¹˜

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv &gt; 0:
        params = {}
        grid = GridSearchCV(model, param_grid=params, cv=cv, n_jobs=-1)
        fit = grid.fit(x_train, y_train)
        model = fit.best_estimator_
        fit.best_params = fit.best_params_
        
        result_df = DataFrame(grid.cv_results_[&#39;params&#39;])
        result_df[&#39;mean_test_score&#39;] = grid.cv_results_[&#39;mean_test_score&#39;]
        
        # print(&#34;[êµì°¨ê²€ì¦]&#34;)
        # my_pretty_table(result_df.sort_values(by=&#39;mean_test_score&#39;, ascending=False))
        # print(&#34;&#34;)

    fit = model.fit(x_train, y_train)
    x = x_test
    y = y_test
    y_pred = fit.predict(x)

    resid = y - y_pred

    # ì ˆí¸ê³¼ ê³„ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
    params = np.append(fit.intercept_, fit.coef_)

    # ê²€ì¦ìš© ë…ë¦½ë³€ìˆ˜ì— ìƒìˆ˜í•­ ì¶”ê°€
    design_x = x.copy()
    design_x.insert(0, &#39;ìƒìˆ˜&#39;, 1)

    dot = np.dot(design_x.T,design_x)   # í–‰ë ¬ê³±
    inv = np.linalg.inv(dot)            # ì—­í–‰ë ¬
    dia = inv.diagonal()                # ëŒ€ê°ì›ì†Œ

    # ì œê³±ì˜¤ì°¨
    MSE = (sum((y-y_pred)**2)) / (len(design_x)-len(design_x.iloc[0]))

    se_b = np.sqrt(MSE * dia)           # í‘œì¤€ì˜¤ì°¨
    ts_b = params / se_b                # tê°’

    # ê° ë…ë¦½ìˆ˜ì— ëŒ€í•œ pvalue
    p_values = [2*(1-t.cdf(np.abs(i),(len(design_x)-len(design_x.iloc[0])))) for i in ts_b]

    # VIF
    if len(x.columns) &gt; 1:
        vif = [variance_inflation_factor(x, list(x.columns).index(v)) for i, v in enumerate(x.columns)]
    else:
        vif = 0

    # í‘œì¤€í™” ê³„ìˆ˜
    train_df = x.copy()
    train_df[y.name] = y
    scaler = StandardScaler()
    std = scaler.fit_transform(train_df)
    std_df = DataFrame(std, columns=train_df.columns)
    std_x = std_df[xnames]
    std_y = std_df[yname]
    std_model = LinearRegression()
    std_fit = std_model.fit(std_x, std_y)
    beta = std_fit.coef_

    # ê²°ê³¼í‘œ êµ¬ì„±í•˜ê¸°
    result_df = DataFrame({
        &#34;ì¢…ì†ë³€ìˆ˜&#34;: [yname] * len(xnames),
        &#34;ë…ë¦½ë³€ìˆ˜&#34;: xnames,
        &#34;B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)&#34;: np.round(params[1:], 4),
        &#34;í‘œì¤€ì˜¤ì°¨&#34;: np.round(se_b[1:], 3),
        &#34;Î²(í‘œì¤€í™” ê³„ìˆ˜)&#34;: np.round(beta, 3),
        &#34;t&#34;: np.round(ts_b[1:], 3),
        &#34;ìœ ì˜í™•ë¥ &#34;: np.round(p_values[1:], 3),
        &#34;VIF&#34;: vif,
    })
    if order:
        order = order.upper()
        if order == &#39;V&#39;:
            result_df.sort_values(&#39;VIF&#39;,inplace=True)
        elif  order == &#39;P&#39;:
            result_df.sort_values(&#39;ìœ ì˜í™•ë¥ &#39;,inplace=True)
        #result_df
    # my_pretty_table(result_df)
        
    resid = y - y_pred        # ì”ì°¨
    dw = durbin_watson(resid)               # ë”ë¹ˆ ì™“ìŠ¨ í†µê³„ëŸ‰
    r2 = r2_score(y, y_pred)  # ê²°ì •ê³„ìˆ˜(ì„¤ëª…ë ¥)
    rowcount = len(x)                # í‘œë³¸ìˆ˜
    featurecount = len(x.columns)    # ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜

    # ë³´ì •ëœ ê²°ì •ê³„ìˆ˜
    adj_r2 = 1 - (1 - r2) * (rowcount-1) / (rowcount-featurecount-1)

    # fê°’
    f_statistic = (r2 / featurecount) / ((1 - r2) / (rowcount - featurecount - 1))

    # Prob (F-statistic)
    p = 1 - f.cdf(f_statistic, featurecount, rowcount - featurecount - 1)

    tpl = f&#34;ğ‘…^2({r2:.3f}), Adj.ğ‘…^2({adj_r2:.3f}), F({f_statistic:.3f}), P-value({p:.4g}), Durbin-Watson({dw:.3f})&#34;
    # print(tpl, end=&#34;\n\n&#34;)

    # ê²°ê³¼ë³´ê³ 
    tpl = f&#34;{yname}ì— ëŒ€í•˜ì—¬ {&#39;,&#39;.join(xnames)}ë¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, ì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜{&#39;í•˜ë‹¤&#39; if p &lt;= 0.05 else &#39;í•˜ì§€ ì•Šë‹¤&#39;}(F({len(x.columns)},{len(x.index)-len(x.columns)-1}) = {f_statistic:0.3f}, p {&#39;&lt;=&#39; if p &lt;= p_value_num else &#39;&gt;&#39;} 0.05).&#34;

    # # print(tpl, end = &#39;\n\n&#39;)

    # ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
    for n in xnames:
        item = result_df[result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;] == n]
        coef = item[&#39;B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)&#39;].values[0]
        pvalue = item[&#39;ìœ ì˜í™•ë¥ &#39;].values[0]

        s = f&#34;{n}ì˜ íšŒê·€ê³„ìˆ˜ëŠ” {coef:0.3f}(p {&#39;&lt;=&#39; if pvalue &lt;= p_value_num else &#39;&gt;&#39;} 0.05)ë¡œ, {yname}ì— ëŒ€í•˜ì—¬ {&#39;ìœ ì˜ë¯¸í•œ&#39; if pvalue &lt;= p_value_num else &#39;ìœ ì˜í•˜ì§€ ì•Šì€&#39;} ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.&#34;

        # print(s)
        
    # print(&#34;&#34;)
    if result_df[&#34;VIF&#34;].max() &gt;= 10:
        # print(&#39;-&#39;*50)
        # print(&#39;ëº€ ë³€ìˆ˜ :&#39;,result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;][result_df[&#39;VIF&#39;].idxmax()])
        # print(&#39;-&#39;*50)
        return my_auto_linear_regrassion(df.drop(result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;][result_df[&#39;VIF&#39;].idxmax()],axis=1), yname, cv, degree,plot,report,resid_test, figsize, dpi, order,p_value_num )
    else:
        if result_df[&#34;ìœ ì˜í™•ë¥ &#34;].max() &gt;= p_value_num:
            # print(&#39;-&#39;*50)
            # print(&#39;ëº€ ë³€ìˆ˜ :&#39;,result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;][result_df[&#39;ìœ ì˜í™•ë¥ &#39;].idxmax()])
            # print(&#39;-&#39;*50)
            return my_auto_linear_regrassion(df.drop(result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;][result_df[&#39;ìœ ì˜í™•ë¥ &#39;].idxmax()],axis=1), yname,cv, degree,plot,report,resid_test, figsize, dpi, order,p_value_num )
    
    x_train, x_test, y_train, y_test = my_train_test_split(df, yname, test_size=0.2)
    
    xnames = x_train.columns
    yname = y_train.name
    size = len(xnames)

    # ë¶„ì„ëª¨ë¸ ìƒì„±
    model = LinearRegression(n_jobs=-1) # n_jobs : ì‚¬ìš©í•˜ëŠ” cpu ì½”ì–´ì˜ ê°œìˆ˜ // -1ì€ ìµœëŒ€ì¹˜

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv &gt; 0:
        params = {}
        grid = GridSearchCV(model, param_grid=params, cv=cv, n_jobs=-1)
        fit = grid.fit(x_train, y_train)
        model = fit.best_estimator_
        fit.best_params = fit.best_params_
        
        result_df = DataFrame(grid.cv_results_[&#39;params&#39;])
        result_df[&#39;mean_test_score&#39;] = grid.cv_results_[&#39;mean_test_score&#39;]
        
        print(&#34;[êµì°¨ê²€ì¦]&#34;)
        my_pretty_table(result_df.sort_values(by=&#39;mean_test_score&#39;, ascending=False))
        print(&#34;&#34;)

    fit = model.fit(x_train, y_train)
    x = x_test
    y = y_test
    y_pred = fit.predict(x)
    expr = &#34;{yname} = &#34;.format(yname=yname)

    for i, v in enumerate(xnames):
        expr += &#34;%0.3f * %s + &#34; % (fit.coef_[i], v)

    expr += &#34;%0.3f&#34; % fit.intercept_
    print(&#34;[íšŒê·€ì‹]&#34;)
    print(expr, end=&#34;\n\n&#34;)
    resid = y - y_pred

    # ì ˆí¸ê³¼ ê³„ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
    params = np.append(fit.intercept_, fit.coef_)

    # ê²€ì¦ìš© ë…ë¦½ë³€ìˆ˜ì— ìƒìˆ˜í•­ ì¶”ê°€
    design_x = x.copy()
    design_x.insert(0, &#39;ìƒìˆ˜&#39;, 1)

    dot = np.dot(design_x.T,design_x)   # í–‰ë ¬ê³±
    inv = np.linalg.inv(dot)            # ì—­í–‰ë ¬
    dia = inv.diagonal()                # ëŒ€ê°ì›ì†Œ

    # ì œê³±ì˜¤ì°¨
    MSE = (sum((y-y_pred)**2)) / (len(design_x)-len(design_x.iloc[0]))

    se_b = np.sqrt(MSE * dia)           # í‘œì¤€ì˜¤ì°¨
    ts_b = params / se_b                # tê°’

    # ê° ë…ë¦½ìˆ˜ì— ëŒ€í•œ pvalue
    p_values = [2*(1-t.cdf(np.abs(i),(len(design_x)-len(design_x.iloc[0])))) for i in ts_b]

    # VIF
    if len(x.columns) &gt; 1:
        vif = [variance_inflation_factor(x, list(x.columns).index(v)) for i, v in enumerate(x.columns)]
    else:
        vif = 0

    # í‘œì¤€í™” ê³„ìˆ˜
    train_df = x.copy()
    train_df[y.name] = y
    scaler = StandardScaler()
    std = scaler.fit_transform(train_df)
    std_df = DataFrame(std, columns=train_df.columns)
    std_x = std_df[xnames]
    std_y = std_df[yname]
    std_model = LinearRegression()
    std_fit = std_model.fit(std_x, std_y)
    beta = std_fit.coef_

    # ê²°ê³¼í‘œ êµ¬ì„±í•˜ê¸°
    result_df = DataFrame({
        &#34;ì¢…ì†ë³€ìˆ˜&#34;: [yname] * len(xnames),
        &#34;ë…ë¦½ë³€ìˆ˜&#34;: xnames,
        &#34;B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)&#34;: np.round(params[1:], 4),
        &#34;í‘œì¤€ì˜¤ì°¨&#34;: np.round(se_b[1:], 3),
        &#34;Î²(í‘œì¤€í™” ê³„ìˆ˜)&#34;: np.round(beta, 3),
        &#34;t&#34;: np.round(ts_b[1:], 3),
        &#34;ìœ ì˜í™•ë¥ &#34;: np.round(p_values[1:], 3),
        &#34;VIF&#34;: vif,
    })
    if order:
        order = order.upper()
        if order == &#39;V&#39;:
            result_df.sort_values(&#39;VIF&#39;,inplace=True)
        elif  order == &#39;P&#39;:
            result_df.sort_values(&#39;ìœ ì˜í™•ë¥ &#39;,inplace=True)
        # result_df
    my_pretty_table(result_df)
        
    resid = y - y_pred        # ì”ì°¨
    dw = durbin_watson(resid)               # ë”ë¹ˆ ì™“ìŠ¨ í†µê³„ëŸ‰
    r2 = r2_score(y, y_pred)  # ê²°ì •ê³„ìˆ˜(ì„¤ëª…ë ¥)
    rowcount = len(x)                # í‘œë³¸ìˆ˜
    featurecount = len(x.columns)    # ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜

    # ë³´ì •ëœ ê²°ì •ê³„ìˆ˜
    adj_r2 = 1 - (1 - r2) * (rowcount-1) / (rowcount-featurecount-1)

    # fê°’
    f_statistic = (r2 / featurecount) / ((1 - r2) / (rowcount - featurecount - 1))

    # Prob (F-statistic)
    p = 1 - f.cdf(f_statistic, featurecount, rowcount - featurecount - 1)

    tpl = f&#34;ğ‘…^2({r2:.3f}), Adj.ğ‘…^2({adj_r2:.3f}), F({f_statistic:.3f}), P-value({p:.4g}), Durbin-Watson({dw:.3f})&#34;
    print(tpl, end=&#34;\n\n&#34;)

    # ê²°ê³¼ë³´ê³ 
    tpl = f&#34;{yname}ì— ëŒ€í•˜ì—¬ {&#39;,&#39;.join(xnames)}ë¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, ì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜{&#39;í•˜ë‹¤&#39; if p &lt;= 0.05 else &#39;í•˜ì§€ ì•Šë‹¤&#39;}(F({len(x.columns)},{len(x.index)-len(x.columns)-1}) = {f_statistic:0.3f}, p {&#39;&lt;=&#39; if p &lt;= p_value_num else &#39;&gt;&#39;} 0.05).&#34;

    print(tpl, end = &#39;\n\n&#39;)

    # ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
    for n in xnames:
        item = result_df[result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;] == n]
        coef = item[&#39;B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)&#39;].values[0]
        pvalue = item[&#39;ìœ ì˜í™•ë¥ &#39;].values[0]

        s = f&#34;{n}ì˜ íšŒê·€ê³„ìˆ˜ëŠ” {coef:0.3f}(p {&#39;&lt;=&#39; if pvalue &lt;= p_value_num else &#39;&gt;&#39;} 0.05)ë¡œ, {yname}ì— ëŒ€í•˜ì—¬ {&#39;ìœ ì˜ë¯¸í•œ&#39; if pvalue &lt;= p_value_num else &#39;ìœ ì˜í•˜ì§€ ì•Šì€&#39;} ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.&#34;

        print(s)
        
    print(&#34;&#34;)
    return fit
    
def my_linear_regrassion(x_train: DataFrame, y_train: Series, x_test: DataFrame = None, y_test: Series = None, cv: int = 5,  learning_curve: bool = True, degree : int = 1, plot: bool = True, report=True, resid_test=False, figsize=(10, 4), dpi=150, sort: str = None,order: str = None,p_value_num:float=0.05 ) -&gt; LinearRegression:
    &#34;&#34;&#34;ì„ í˜•íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 5.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 4).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 150.
        order (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        p_value_num (float, optional) : íšŒê·€ëª¨í˜•ì˜ ìœ ì˜í™•ë¥ . Drfaults to 0.05
    Returns:
        LinearRegression: íšŒê·€ë¶„ì„ ëª¨ë¸
    &#34;&#34;&#34;
    xnames = x_train.columns
    yname = y_train.name
    size = len(xnames)

    # ë¶„ì„ëª¨ë¸ ìƒì„±

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv &gt; 0:
        params = {}
        prototype_estimator = LinearRegression(n_jobs=-1)
        grid = GridSearchCV(prototype_estimator, param_grid=params, cv=cv, n_jobs=-1)
        grid.fit(x_train, y_train)
        result_df = DataFrame(grid.cv_results_[&#39;params&#39;])
        result_df[&#39;mean_test_score&#39;] = grid.cv_results_[&#39;mean_test_score&#39;]
        print(&#34;[êµì°¨ê²€ì¦]&#34;)
        my_pretty_table(result_df.sort_values(by=&#39;mean_test_score&#39;, ascending=False))
        print(&#34;&#34;)

        estimator = grid.best_estimator_
        estimator.best_params = grid.best_params_
    else:
        estimator = LinearRegression(n_jobs=-1)
        estimator.fit(x_train, y_train)        
    y_pred = estimator.predict(x_test) if x_test is not None else estimator.predict(x_train)
    
    # ë„ì¶œëœ ê²°ê³¼ë¥¼ íšŒê·€ëª¨ë¸ ê°ì²´ì— í¬í•¨ì‹œí‚´
    estimator.x = x_test if x_test is not None else x_train
    estimator.y = y_test if y_test is not None else y_train
    estimator.y_pred = y_pred if y_test is not None else estimator.predict(x_train)
    estimator.resid = y_test - y_pred if y_test is not None else y_train - estimator.predict(x_train)

    #------------------------------------------------------
    # ì„±ëŠ¥í‰ê°€
    if x_test is not None and y_test is not None:
        my_regrassion_result(estimator, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, learning_curve=learning_curve, cv=cv, figsize=figsize, dpi=dpi)
    else:
        my_regrassion_result(estimator, x_train=x_train, y_train=y_train, learning_curve=learning_curve, cv=cv, figsize=figsize, dpi=dpi)

    #------------------------------------------------------
    # ë³´ê³ ì„œ ì¶œë ¥
    if report:
        print(&#34;&#34;)
        my_regrassion_report(estimator, estimator.x, estimator.y, sort=sort, plot=plot, degree=degree, figsize=figsize, dpi=dpi)
    
    #------------------------------------------------------
    # ì”ì°¨ ê°€ì • í™•ì¸  
    if resid_test:
        print(&#34;\n\n[ì”ì°¨ì˜ ê°€ì • í™•ì¸] ==============================&#34;)
        my_resid_test(estimator.x, estimator.y, estimator.y_pred, figsize=figsize, dpi=dpi)

    return estimator

def my_ridge_regrassion(x_train: DataFrame, y_train: Series, x_test: DataFrame = None, y_test: Series = None, cv: int = 5, learning_curve: bool = True, report=False, plot: bool = False, degree: int = 1, resid_test=False, figsize=(10, 5), dpi: int = 100, sort: str = None, params: dict = {&#39;alpha&#39;: [0.01, 0.1, 1, 10, 100]}) -&gt; LinearRegression:
    &#34;&#34;&#34;ë¦¿ì§€íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 5.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to {&#39;alpha&#39;: [0.01, 0.1, 1, 10, 100]}.
        
    Returns:
        Ridge: Ridge ëª¨ë¸
    &#34;&#34;&#34;
    
    #------------------------------------------------------
    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv &gt; 0:   
        # ë¶„ì„ëª¨ë¸ ìƒì„±
        prototype_estimator = Ridge()     
        
        print(&#34;[%s í•˜ì´í¼íŒŒë¼ë¯¸í„°]&#34; % prototype_estimator.__class__.__name__)
        my_pretty_table(DataFrame(params))
        print(&#34;&#34;)
        
        grid = GridSearchCV(prototype_estimator, param_grid=params, cv=cv, n_jobs=-1)
        grid.fit(x_train, y_train)
        
        result_df = DataFrame(grid.cv_results_[&#39;params&#39;])
        result_df[&#39;mean_test_score&#39;] = grid.cv_results_[&#39;mean_test_score&#39;]
        
        print(&#34;[êµì°¨ê²€ì¦]&#34;)
        my_pretty_table(result_df.sort_values(by=&#39;mean_test_score&#39;, ascending=False))
        print(&#34;&#34;)
        
        estimator = grid.best_estimator_
        estimator.best_params = grid.best_params_
    else:
        # ë¶„ì„ëª¨ë¸ ìƒì„±
        estimator = Ridge(**params) 
        estimator.fit(x_train, y_train)
    
    #------------------------------------------------------
    xnames = x_train.columns
    yname = y_train.name
    
    # í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ì¶”ì •ì¹˜ ìƒì„±
    y_pred = estimator.predict(x_test) if x_test is not None else estimator.predict(x_train)
    
    # ë„ì¶œëœ ê²°ê³¼ë¥¼ íšŒê·€ëª¨ë¸ ê°ì²´ì— í¬í•¨ì‹œí‚´
    estimator.x = x_test if x_test is not None else x_train
    estimator.y = y_test if y_test is not None else y_train
    estimator.y_pred = y_pred if y_test is not None else estimator.predict(x_train)
    estimator.resid = y_test - y_pred if y_test is not None else y_train - estimator.predict(x_train)

    #------------------------------------------------------
    # ì„±ëŠ¥í‰ê°€
    if x_test is not None and y_test is not None:
        my_regrassion_result(estimator, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, learning_curve=learning_curve, cv=cv, figsize=figsize, dpi=dpi)
    else:
        my_regrassion_result(estimator, x_train=x_train, y_train=y_train, learning_curve=learning_curve, cv=cv, figsize=figsize, dpi=dpi)

    #------------------------------------------------------
    # ë³´ê³ ì„œ ì¶œë ¥
    if report:
        print(&#34;&#34;)
        my_regrassion_report(estimator, estimator.x, estimator.y, sort, plot=plot, degree=degree, figsize=figsize, dpi=dpi)
    
    #------------------------------------------------------
    # ì”ì°¨ ê°€ì • í™•ì¸  
    if resid_test:
        print(&#34;\n\n[ì”ì°¨ì˜ ê°€ì • í™•ì¸] ==============================&#34;)
        my_resid_test(estimator.x, estimator.y, estimator.y_pred, figsize=figsize, dpi=dpi)

    return estimator

def my_lasso_regrassion(x_train: DataFrame, y_train: Series, x_test: DataFrame = None, y_test: Series = None, cv: int = 5, learning_curve: bool = True, report=False, plot: bool = False, degree: int = 1, resid_test=False, figsize=(10, 5), dpi: int = 100, sort: str = None, params: dict = {&#39;alpha&#39;: [0.01, 0.1, 1, 10, 100]}) -&gt; LinearRegression:
    &#34;&#34;&#34;ë¼ì˜íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 5.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to {&#39;alpha&#39;: [0.01, 0.1, 1, 10, 100]}.
        
    Returns:
        Lasso: Lasso ëª¨ë¸
    &#34;&#34;&#34;
    
    #------------------------------------------------------
    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv &gt; 0:   
        # ë¶„ì„ëª¨ë¸ ìƒì„±
        prototype_estimator = Lasso()     
        
        print(&#34;[%s í•˜ì´í¼íŒŒë¼ë¯¸í„°]&#34; % prototype_estimator.__class__.__name__)
        my_pretty_table(DataFrame(params))
        print(&#34;&#34;)
        
        grid = GridSearchCV(prototype_estimator, param_grid=params, cv=cv, n_jobs=-1)
        grid.fit(x_train, y_train)
        
        result_df = DataFrame(grid.cv_results_[&#39;params&#39;])
        result_df[&#39;mean_test_score&#39;] = grid.cv_results_[&#39;mean_test_score&#39;]
        
        print(&#34;[êµì°¨ê²€ì¦]&#34;)
        my_pretty_table(result_df.sort_values(by=&#39;mean_test_score&#39;, ascending=False))
        print(&#34;&#34;)
        
        estimator = grid.best_estimator_
        estimator.best_params = grid.best_params_
    else:
        # ë¶„ì„ëª¨ë¸ ìƒì„±
        estimator = Lasso(**params) 
        estimator.fit(x_train, y_train)
    
    #------------------------------------------------------
    xnames = x_train.columns
    yname = y_train.name
    
    # í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ì¶”ì •ì¹˜ ìƒì„±
    y_pred = estimator.predict(x_test) if x_test is not None else estimator.predict(x_train)
    
    # ë„ì¶œëœ ê²°ê³¼ë¥¼ íšŒê·€ëª¨ë¸ ê°ì²´ì— í¬í•¨ì‹œí‚´
    estimator.x = x_test if x_test is not None else x_train
    estimator.y = y_test if y_test is not None else y_train
    estimator.y_pred = y_pred if y_test is not None else estimator.predict(x_train)
    estimator.resid = y_test - y_pred if y_test is not None else y_train - estimator.predict(x_train)

    #------------------------------------------------------
    # ì„±ëŠ¥í‰ê°€
    if x_test is not None and y_test is not None:
        my_regrassion_result(estimator, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, learning_curve=learning_curve, cv=cv, figsize=figsize, dpi=dpi)
    else:
        my_regrassion_result(estimator, x_train=x_train, y_train=y_train, learning_curve=learning_curve, cv=cv, figsize=figsize, dpi=dpi)

    #------------------------------------------------------
    # ë³´ê³ ì„œ ì¶œë ¥
    if report:
        print(&#34;&#34;)
        my_regrassion_report(estimator, estimator.x, estimator.y, sort, plot=plot, degree=degree, figsize=figsize, dpi=dpi)
    
    #------------------------------------------------------
    # ì”ì°¨ ê°€ì • í™•ì¸  
    if resid_test:
        print(&#34;\n\n[ì”ì°¨ì˜ ê°€ì • í™•ì¸] ==============================&#34;)
        my_resid_test(estimator.x, estimator.y, estimator.y_pred, figsize=figsize, dpi=dpi)

    return estimator

def my_regrassion_result(estimator: any, x_train: DataFrame = None, y_train: Series = None, x_test: DataFrame = None, y_test: Series = None, learning_curve: bool = True, cv: int = 10, figsize: tuple = (10, 5), dpi: int = 100) -&gt; None:
    &#34;&#34;&#34;íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        estimator (any): íšŒê·€ë¶„ì„ ëª¨ë¸
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 10.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
    &#34;&#34;&#34;
    
    scores = []
    score_names = []
    
    if x_train is not None and y_train is not None:
        y_train_pred = estimator.predict(x_train)

        # ì„±ëŠ¥í‰ê°€
        result = {
            &#34;ê²°ì •ê³„ìˆ˜(R2)&#34;: r2_score(y_train, y_train_pred),
            &#34;í‰ê· ì ˆëŒ€ì˜¤ì°¨(MAE)&#34;: mean_absolute_error(y_train, y_train_pred),
            &#34;í‰ê· ì œê³±ì˜¤ì°¨(MSE)&#34;: mean_squared_error(y_train, y_train_pred),
            &#34;í‰ê· ì˜¤ì°¨(RMSE)&#34;: np.sqrt(mean_squared_error(y_train, y_train_pred)),
            &#34;í‰ê·  ì ˆëŒ€ ë°±ë¶„ì˜¤ì°¨ ë¹„ìœ¨(MAPE)&#34;: np.mean(np.abs((y_train - y_train_pred) / y_train) * 100),
            &#34;í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨(MPE)&#34;: np.mean((y_train - y_train_pred) / y_train * 100)
        }
        
        scores.append(result)
        score_names.append(&#34;í›ˆë ¨ë°ì´í„°&#34;)
        
    if x_test is not None and y_test is not None:
        y_test_pred = estimator.predict(x_test)

        # ì„±ëŠ¥í‰ê°€
        result = {
            &#34;ê²°ì •ê³„ìˆ˜(R2)&#34;: r2_score(y_test, y_test_pred),
            &#34;í‰ê· ì ˆëŒ€ì˜¤ì°¨(MAE)&#34;: mean_absolute_error(y_test, y_test_pred),
            &#34;í‰ê· ì œê³±ì˜¤ì°¨(MSE)&#34;: mean_squared_error(y_test, y_test_pred),
            &#34;í‰ê· ì˜¤ì°¨(RMSE)&#34;: np.sqrt(mean_squared_error(y_test, y_test_pred)),
            &#34;í‰ê·  ì ˆëŒ€ ë°±ë¶„ì˜¤ì°¨ ë¹„ìœ¨(MAPE)&#34;: np.mean(np.abs((y_test - y_test_pred) / y_test) * 100),
            &#34;í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨(MPE)&#34;: np.mean((y_test - y_test_pred) / y_test * 100)
        }
        
        scores.append(result)
        score_names.append(&#34;ê²€ì¦ë°ì´í„°&#34;)
        

    print(&#34;[íšŒê·€ë¶„ì„ ì„±ëŠ¥í‰ê°€]&#34;)
    result_df = DataFrame(scores, index=score_names)
    my_pretty_table(result_df.T)
    
    # í•™ìŠµê³¡ì„ 
    if learning_curve:
        print(&#34;\n[í•™ìŠµê³¡ì„ ]&#34;)
        yname = y_train.name
        
        if x_test is not None and y_test is not None:
            y_df = concat([y_train, y_test])
            x_df = concat([x_train, x_test])
        else:
            y_df = y_train.copy()
            x_df = x_train.copy()
            
        x_df[yname] = y_df 
        x_df.sort_index(inplace=True)
        
        if cv &gt; 0:
            my_learing_curve(estimator, data=x_df, yname=yname, cv=cv, scoring=&#39;RMSE&#39;, figsize=figsize, dpi=dpi)
        else:
            my_learing_curve(estimator, data=x_df, yname=yname, scoring=&#39;RMSE&#39;, figsize=figsize, dpi=dpi)

def my_regrassion_report(estimator: any, x: DataFrame = None, y: Series = None, sort: str = None, plot: bool = False, degree: int = 1, figsize: tuple = (10, 5), dpi: int = 100, order : str = None, p_value_num:float=0.05 ) -&gt; None:
    &#34;&#34;&#34;ì„ í˜•íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ í•œë‹¤.

    Args:
        fit (LinearRegression): ì„ í˜•íšŒê·€ ê°ì²´
        x (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        sort (str, optional): ì •ë ¬ ê¸°ì¤€ (v, p). Defaults to None.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to False.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        order (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        p_value_num (float, optional) : íšŒê·€ëª¨í˜•ì˜ ìœ ì˜í™•ë¥ . Drfaults to 0.05
    &#34;&#34;&#34;
    
    # íšŒê·€ì‹
    xnames = x.columns
    yname = y.name
    
    expr = &#34;{yname} = &#34;.format(yname=yname)

    for i, v in enumerate(xnames):
        expr += &#34;%0.3f * %s + &#34; % (estimator.coef_[i], v)

    expr += &#34;%0.3f&#34; % estimator.intercept_
    print(&#34;[íšŒê·€ì‹]&#34;)
    print(expr, end=&#34;\n\n&#34;)
    
    
    print(&#34;[ë…ë¦½ë³€ìˆ˜ë³´ê³ ]&#34;)
    if x is None and y is None:
        x = estimator.x
        y = estimator.y
    
    y_pred = estimator.predict(x)
    xnames = x.columns
    yname = y.name

    # ì”ì°¨
    resid = y - y_pred

    # ì ˆí¸ê³¼ ê³„ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
    params = np.append(estimator.intercept_, estimator.coef_)

    # ê²€ì¦ìš© ë…ë¦½ë³€ìˆ˜ì— ìƒìˆ˜í•­ ì¶”ê°€
    design_x = x.copy()
    design_x.insert(0, &#39;ìƒìˆ˜&#39;, 1)

    dot = np.dot(design_x.T,design_x)   # í–‰ë ¬ê³±
    inv = np.linalg.inv(dot)            # ì—­í–‰ë ¬
    dia = inv.diagonal()                # ëŒ€ê°ì›ì†Œ

    # ì œê³±ì˜¤ì°¨
    MSE = (sum((y-y_pred)**2)) / (len(design_x)-len(design_x.iloc[0]))

    se_b = np.sqrt(MSE * dia)           # í‘œì¤€ì˜¤ì°¨
    ts_b = params / se_b                # tê°’

    # ê° ë…ë¦½ìˆ˜ì— ëŒ€í•œ pvalue
    p_values = [2*(1-t.cdf(np.abs(i),(len(design_x)-len(design_x.iloc[0])))) for i in ts_b]

    # VIF
    if len(x.columns) &gt; 1:
        vif = [variance_inflation_factor(x, list(x.columns).index(v)) for i, v in enumerate(x.columns)]
    else:
        vif = 0

    # í‘œì¤€í™” ê³„ìˆ˜
    train_df = x.copy()
    train_df[y.name] = y
    scaler = StandardScaler()
    std = scaler.fit_transform(train_df)
    std_df = DataFrame(std, columns=train_df.columns)
    std_x = std_df[xnames]
    std_y = std_df[yname]
    std_estimator = LinearRegression(n_jobs=-1)
    std_estimator.fit(std_x, std_y)
    beta = std_estimator.coef_

    # ê²°ê³¼í‘œ êµ¬ì„±í•˜ê¸°
    result_df = DataFrame({
        &#34;ì¢…ì†ë³€ìˆ˜&#34;: [yname] * len(xnames),
        &#34;ë…ë¦½ë³€ìˆ˜&#34;: xnames,
        &#34;B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)&#34;: np.round(params[1:], 4),
        &#34;í‘œì¤€ì˜¤ì°¨&#34;: np.round(se_b[1:], 3),
        &#34;Î²(í‘œì¤€í™” ê³„ìˆ˜)&#34;: np.round(beta, 3),
        &#34;t&#34;: np.round(ts_b[1:], 3),
        &#34;ìœ ì˜í™•ë¥ &#34;: np.round(p_values[1:], 3),
        &#34;VIF&#34;: vif,
    })
    
    if sort:
        if sort.upper() == &#39;V&#39;:
            result_df.sort_values(&#39;VIF&#39;, inplace=True)
        elif sort.upper() == &#39;P&#39;:
            result_df.sort_values(&#39;ìœ ì˜í™•ë¥ &#39;, inplace=True)
    

    #result_df
    my_pretty_table(result_df)
    print(&#34;&#34;)

    resid = y - y_pred        # ì”ì°¨
    dw = durbin_watson(resid)               # ë”ë¹ˆ ì™“ìŠ¨ í†µê³„ëŸ‰
    r2 = r2_score(y, y_pred)  # ê²°ì •ê³„ìˆ˜(ì„¤ëª…ë ¥)
    rowcount = len(x)                # í‘œë³¸ìˆ˜
    featurecount = len(x.columns)    # ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜

    # ë³´ì •ëœ ê²°ì •ê³„ìˆ˜
    adj_r2 = 1 - (1 - r2) * (rowcount-1) / (rowcount-featurecount-1)

    # fê°’
    f_statistic = (r2 / featurecount) / ((1 - r2) / (rowcount - featurecount - 1))

    # Prob (F-statistic)
    p = 1 - f.cdf(f_statistic, featurecount, rowcount - featurecount - 1)

    tpl = &#34;ğ‘…^2(%.3f), Adj.ğ‘…^2(%.3f), F(%.3f), P-value(%.4g), Durbin-Watson(%.3f)&#34;
    print(tpl % (r2, adj_r2, f_statistic, p, dw), end=&#34;\n\n&#34;)

    # ê²°ê³¼ë³´ê³ 
    tpl = &#34;%sì— ëŒ€í•˜ì—¬ %së¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼,\nì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ %s(F(%s,%s) = %0.3f, p %s %s).&#34;

    result_str = tpl % (
        yname,
        &#34;,&#34;.join(xnames),
        &#34;ìœ ì˜í•˜ë‹¤&#34; if p &lt;= p_value_num else &#34;ìœ ì˜í•˜ì§€ ì•Šë‹¤&#34;,
        len(x.columns),
        len(x.index)-len(x.columns)-1,
        f_statistic,
        &#34;&lt;=&#34; if p &lt;= p_value_num else &#34;&gt;&#34;,
        p_value_num)
        
    print(result_str, end=&#34;\n\n&#34;)

    # ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
    for n in xnames:
        item = result_df[result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;] == n]
        coef = item[&#39;B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)&#39;].values[0]
        pvalue = item[&#39;ìœ ì˜í™•ë¥ &#39;].values[0]

        s = &#34;%sì˜ íšŒê·€ê³„ìˆ˜ëŠ” %0.3f(p %s %s)ë¡œ, %sì— ëŒ€í•˜ì—¬ %s.&#34;
        k = s % (n,
                coef,
                &#34;&lt;=&#34; if pvalue &lt;= p_value_num else &#39;&gt;&#39;,
                yname,
                &#39;ìœ ì˜ë¯¸í•œ ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤&#39; if pvalue &lt;= p_value_num else &#39;ìœ ì˜í•˜ì§€ ì•Šì€ ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤&#39;,
                p_value_num
        )

        print(k)
        
    # ë„ì¶œëœ ê²°ê³¼ë¥¼ íšŒê·€ëª¨ë¸ ê°ì²´ì— í¬í•¨ì‹œí‚´ --&gt; ê°ì²´ íƒ€ì…ì˜ íŒŒë¼ë¯¸í„°ëŠ” ì°¸ì¡°ë³€ìˆ˜ë¡œ ì „ë‹¬ë˜ë¯€ë¡œ fit ê°ì²´ì— í¬í•¨ëœ ê²°ê³¼ê°’ë“¤ì€ ì´ í•¨ìˆ˜ ì™¸ë¶€ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.
    estimator.r2 = r2
    estimator.adj_r2 = adj_r2
    estimator.f_statistic = f_statistic
    estimator.p = p
    estimator.dw = dw
        
    # ì‹œê°í™”
    if plot:
        for i, v in enumerate(xnames):
            plt.figure(figsize=figsize, dpi=dpi)
            
            if degree == 1:
                sb.regplot(x=x[v], y=y, ci=95, label=&#39;ê´€ì¸¡ì¹˜&#39;)
                sb.regplot(x=x[v], y=y_pred, ci=0, label=&#39;ì¶”ì •ì¹˜&#39;)
            else:
                sb.scatterplot(x=x[v], y=y, label=&#39;ê´€ì¸¡ì¹˜&#39;)
                sb.scatterplot(x=x[v], y=y_pred, label=&#39;ì¶”ì •ì¹˜&#39;)
                
                t1 = my_trend(x[v], y, degree=degree)
                sb.lineplot(x=t1[0], y=t1[1], color=&#39;blue&#39;, linestyle=&#39;--&#39;, label=&#39;ê´€ì¸¡ì¹˜ ì¶”ì„¸ì„ &#39;)
                
                t2 = my_trend(x[v], y_pred, degree=degree)
                sb.lineplot(x=t2[0], y=t2[1], color=&#39;red&#39;, linestyle=&#39;--&#39;, label=&#39;ì¶”ì •ì¹˜ ì¶”ì„¸ì„ &#39;)
            
            plt.title(f&#34;{yname} vs {v}&#34;)
            plt.legend()
            plt.grid()

            plt.show()
            plt.close()
        
def my_resid_normality(y: Series, y_pred: Series) -&gt; None:
    &#34;&#34;&#34;MSEê°’ì„ ì´ìš©í•˜ì—¬ ì”ì°¨ì˜ ì •ê·œì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
    &#34;&#34;&#34;
    mse = mean_squared_error(y, y_pred)
    resid = y - y_pred
    mse_sq = np.sqrt(mse)

    r1 = resid[ (resid &gt; -mse_sq) &amp; (resid &lt; mse_sq)].count() / resid.count() * 100
    r2 = resid[ (resid &gt; -2*mse_sq) &amp; (resid &lt; 2*mse_sq)].count() / resid.count() * 100
    r3 = resid[ (resid &gt; -3*mse_sq) &amp; (resid &lt; 3*mse_sq)].count() / resid.count() * 100

    mse_r = [r1, r2, r3]
    
    print(f&#34;ë£¨íŠ¸ 1MSE êµ¬ê°„ì— í¬í•¨ëœ ì”ì°¨ ë¹„ìœ¨: {r1:1.2f}% ({r1-68})&#34;)
    print(f&#34;ë£¨íŠ¸ 2MSE êµ¬ê°„ì— í¬í•¨ëœ ì”ì°¨ ë¹„ìœ¨: {r2:1.2f}% ({r2-95})&#34;)
    print(f&#34;ë£¨íŠ¸ 3MSE êµ¬ê°„ì— í¬í•¨ëœ ì”ì°¨ ë¹„ìœ¨: {r3:1.2f}% ({r3-99})&#34;)
    
    normality = r1 &gt;= 68 and r2 &gt;= 95 and r3 &gt;= 99
    print(f&#34;ì”ì°¨ì˜ ì •ê·œì„± ê°€ì • ì¶©ì¡± ì—¬ë¶€: {normality}&#34;)

def my_resid_equal_var(x: DataFrame, y: Series, y_pred: Series, p_value_num:float =0.05) -&gt; None:
    &#34;&#34;&#34;ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        x (DataFrame): ë…ë¦½ë³€ìˆ˜
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
        p_value_num(float) : ìœ ì˜í™•ë¥ 
    &#34;&#34;&#34;
    # ë…ë¦½ë³€ìˆ˜ ë°ì´í„° í”„ë ˆì„ ë³µì‚¬
    x_copy = x.copy()
    
    # ìƒìˆ˜í•­ ì¶”ê°€
    x_copy.insert(0, &#34;const&#34;, 1)
    
    # ì”ì°¨ êµ¬í•˜ê¸°
    resid = y - y_pred
    
    # ë“±ë¶„ì‚°ì„± ê²€ì •
    bs_result = het_breuschpagan(resid, x_copy)
    bs_result_df = DataFrame(bs_result, columns=[&#39;values&#39;], index=[&#39;statistic&#39;, &#39;p-value&#39;, &#39;f-value&#39;, &#39;f p-value&#39;])

    print(f&#34;ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± ê°€ì • ì¶©ì¡± ì—¬ë¶€: {bs_result[1] &gt; p_value_num}&#34;)
    my_pretty_table(bs_result_df)

def my_resid_independence(y: Series, y_pred: Series) -&gt; None:
    &#34;&#34;&#34;ì”ì°¨ì˜ ë…ë¦½ì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
    &#34;&#34;&#34;
    dw = durbin_watson(y - y_pred)
    print(f&#34;Durbin-Watson: {dw}, ì”ì°¨ì˜ ë…ë¦½ì„± ê°€ì • ë§Œì¡± ì—¬ë¶€: {1.5 &lt; dw &lt; 2.5}&#34;)
    
def my_resid_test(x: DataFrame, y: Series, y_pred: Series, figsize: tuple=(10, 4), dpi: int=150, p_value_num:float = 0.05) -&gt; None:
    &#34;&#34;&#34;ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        x (Series): ë…ë¦½ë³€ìˆ˜
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
        p_value_num(float) : ìœ ì˜í™•ë¥ 
    &#34;&#34;&#34;

    # ì”ì°¨ ìƒì„±
    resid = y - y_pred
    
    print(&#34;[ì”ì°¨ì˜ ì„ í˜•ì„± ê°€ì •]&#34;)
    my_residplot(y, y_pred, lowess=True, figsize=figsize, dpi=dpi)
    
    print(&#34;\n[ì”ì°¨ì˜ ì •ê·œì„± ê°€ì •]&#34;)
    my_qqplot(y, figsize=figsize, dpi=dpi)
    my_residplot(y, y_pred, mse=True, figsize=figsize, dpi=dpi)
    my_resid_normality(y, y_pred)
    
    print(&#34;\n[ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± ê°€ì •]&#34;)
    my_resid_equal_var(x, y, y_pred, p_value_num)
    
    print(&#34;\n[ì”ì°¨ì˜ ë…ë¦½ì„± ê°€ì •]&#34;)
    my_resid_independence(y, y_pred)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="helper.regrassion.my_auto_linear_regrassion"><code class="name flex">
<span>def <span class="ident">my_auto_linear_regrassion</span></span>(<span>df:Â pandas.core.frame.DataFrame, yname:Â str, cv:Â intÂ =Â 5, learning_curve:Â boolÂ =Â True, degree:Â intÂ =Â 1, plot:Â boolÂ =Â True, report=True, resid_test=False, figsize=(10, 4), dpi=150, sort:Â strÂ =Â None, order:Â strÂ =Â None, p_value_num:Â floatÂ =Â 0.05) â€‘>Â sklearn.linear_model._base.LinearRegression</span>
</code></dt>
<dd>
<div class="desc"><p>ì„ í˜•íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.</p>
<h2 id="args">Args</h2>
<dl>
<dt>df (DataFrame) : íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•  ë°ì´í„°í”„ë ˆì„.</dt>
<dt>yname (str) : ì¢…ì†ë³€ìˆ˜</dt>
<dt><strong><code>cv</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.</dd>
<dt><strong><code>degree</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ì‹œê°í™” ì—¬ë¶€. Defaults to True.</dd>
<dt><strong><code>report</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.</dd>
<dt><strong><code>resid_test</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 4).</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 150.</dd>
<dt><strong><code>order</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)</dd>
</dl>
<p>p_value_num (float, optional) : íšŒê·€ëª¨í˜•ì˜ ìœ ì˜í™•ë¥ . Drfaults to 0.05</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>LinearRegression</code></dt>
<dd>íšŒê·€ë¶„ì„ ëª¨ë¸</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def my_auto_linear_regrassion(df:DataFrame, yname:str, cv:int=5, learning_curve: bool = True, degree : int = 1, plot: bool = True, report=True, resid_test=False, figsize=(10, 4), dpi=150, sort: str = None,order: str = None,p_value_num:float=0.05) -&gt; LinearRegression:
    &#34;&#34;&#34;ì„ í˜•íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        df (DataFrame) : íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•  ë°ì´í„°í”„ë ˆì„.
        yname (str) : ì¢…ì†ë³€ìˆ˜
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 0.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 4).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 150.
        order (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        p_value_num (float, optional) : íšŒê·€ëª¨í˜•ì˜ ìœ ì˜í™•ë¥ . Drfaults to 0.05
    Returns:
        LinearRegression: íšŒê·€ë¶„ì„ ëª¨ë¸
    &#34;&#34;&#34;

    x_train, x_test, y_train, y_test = my_train_test_split(df, yname, test_size=0.2)
    
    xnames = x_train.columns
    yname = y_train.name
    size = len(xnames)

    # ë¶„ì„ëª¨ë¸ ìƒì„±
    model = LinearRegression(n_jobs=-1) # n_jobs : ì‚¬ìš©í•˜ëŠ” cpu ì½”ì–´ì˜ ê°œìˆ˜ // -1ì€ ìµœëŒ€ì¹˜

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv &gt; 0:
        params = {}
        grid = GridSearchCV(model, param_grid=params, cv=cv, n_jobs=-1)
        fit = grid.fit(x_train, y_train)
        model = fit.best_estimator_
        fit.best_params = fit.best_params_
        
        result_df = DataFrame(grid.cv_results_[&#39;params&#39;])
        result_df[&#39;mean_test_score&#39;] = grid.cv_results_[&#39;mean_test_score&#39;]
        
        # print(&#34;[êµì°¨ê²€ì¦]&#34;)
        # my_pretty_table(result_df.sort_values(by=&#39;mean_test_score&#39;, ascending=False))
        # print(&#34;&#34;)

    fit = model.fit(x_train, y_train)
    x = x_test
    y = y_test
    y_pred = fit.predict(x)

    resid = y - y_pred

    # ì ˆí¸ê³¼ ê³„ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
    params = np.append(fit.intercept_, fit.coef_)

    # ê²€ì¦ìš© ë…ë¦½ë³€ìˆ˜ì— ìƒìˆ˜í•­ ì¶”ê°€
    design_x = x.copy()
    design_x.insert(0, &#39;ìƒìˆ˜&#39;, 1)

    dot = np.dot(design_x.T,design_x)   # í–‰ë ¬ê³±
    inv = np.linalg.inv(dot)            # ì—­í–‰ë ¬
    dia = inv.diagonal()                # ëŒ€ê°ì›ì†Œ

    # ì œê³±ì˜¤ì°¨
    MSE = (sum((y-y_pred)**2)) / (len(design_x)-len(design_x.iloc[0]))

    se_b = np.sqrt(MSE * dia)           # í‘œì¤€ì˜¤ì°¨
    ts_b = params / se_b                # tê°’

    # ê° ë…ë¦½ìˆ˜ì— ëŒ€í•œ pvalue
    p_values = [2*(1-t.cdf(np.abs(i),(len(design_x)-len(design_x.iloc[0])))) for i in ts_b]

    # VIF
    if len(x.columns) &gt; 1:
        vif = [variance_inflation_factor(x, list(x.columns).index(v)) for i, v in enumerate(x.columns)]
    else:
        vif = 0

    # í‘œì¤€í™” ê³„ìˆ˜
    train_df = x.copy()
    train_df[y.name] = y
    scaler = StandardScaler()
    std = scaler.fit_transform(train_df)
    std_df = DataFrame(std, columns=train_df.columns)
    std_x = std_df[xnames]
    std_y = std_df[yname]
    std_model = LinearRegression()
    std_fit = std_model.fit(std_x, std_y)
    beta = std_fit.coef_

    # ê²°ê³¼í‘œ êµ¬ì„±í•˜ê¸°
    result_df = DataFrame({
        &#34;ì¢…ì†ë³€ìˆ˜&#34;: [yname] * len(xnames),
        &#34;ë…ë¦½ë³€ìˆ˜&#34;: xnames,
        &#34;B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)&#34;: np.round(params[1:], 4),
        &#34;í‘œì¤€ì˜¤ì°¨&#34;: np.round(se_b[1:], 3),
        &#34;Î²(í‘œì¤€í™” ê³„ìˆ˜)&#34;: np.round(beta, 3),
        &#34;t&#34;: np.round(ts_b[1:], 3),
        &#34;ìœ ì˜í™•ë¥ &#34;: np.round(p_values[1:], 3),
        &#34;VIF&#34;: vif,
    })
    if order:
        order = order.upper()
        if order == &#39;V&#39;:
            result_df.sort_values(&#39;VIF&#39;,inplace=True)
        elif  order == &#39;P&#39;:
            result_df.sort_values(&#39;ìœ ì˜í™•ë¥ &#39;,inplace=True)
        #result_df
    # my_pretty_table(result_df)
        
    resid = y - y_pred        # ì”ì°¨
    dw = durbin_watson(resid)               # ë”ë¹ˆ ì™“ìŠ¨ í†µê³„ëŸ‰
    r2 = r2_score(y, y_pred)  # ê²°ì •ê³„ìˆ˜(ì„¤ëª…ë ¥)
    rowcount = len(x)                # í‘œë³¸ìˆ˜
    featurecount = len(x.columns)    # ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜

    # ë³´ì •ëœ ê²°ì •ê³„ìˆ˜
    adj_r2 = 1 - (1 - r2) * (rowcount-1) / (rowcount-featurecount-1)

    # fê°’
    f_statistic = (r2 / featurecount) / ((1 - r2) / (rowcount - featurecount - 1))

    # Prob (F-statistic)
    p = 1 - f.cdf(f_statistic, featurecount, rowcount - featurecount - 1)

    tpl = f&#34;ğ‘…^2({r2:.3f}), Adj.ğ‘…^2({adj_r2:.3f}), F({f_statistic:.3f}), P-value({p:.4g}), Durbin-Watson({dw:.3f})&#34;
    # print(tpl, end=&#34;\n\n&#34;)

    # ê²°ê³¼ë³´ê³ 
    tpl = f&#34;{yname}ì— ëŒ€í•˜ì—¬ {&#39;,&#39;.join(xnames)}ë¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, ì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜{&#39;í•˜ë‹¤&#39; if p &lt;= 0.05 else &#39;í•˜ì§€ ì•Šë‹¤&#39;}(F({len(x.columns)},{len(x.index)-len(x.columns)-1}) = {f_statistic:0.3f}, p {&#39;&lt;=&#39; if p &lt;= p_value_num else &#39;&gt;&#39;} 0.05).&#34;

    # # print(tpl, end = &#39;\n\n&#39;)

    # ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
    for n in xnames:
        item = result_df[result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;] == n]
        coef = item[&#39;B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)&#39;].values[0]
        pvalue = item[&#39;ìœ ì˜í™•ë¥ &#39;].values[0]

        s = f&#34;{n}ì˜ íšŒê·€ê³„ìˆ˜ëŠ” {coef:0.3f}(p {&#39;&lt;=&#39; if pvalue &lt;= p_value_num else &#39;&gt;&#39;} 0.05)ë¡œ, {yname}ì— ëŒ€í•˜ì—¬ {&#39;ìœ ì˜ë¯¸í•œ&#39; if pvalue &lt;= p_value_num else &#39;ìœ ì˜í•˜ì§€ ì•Šì€&#39;} ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.&#34;

        # print(s)
        
    # print(&#34;&#34;)
    if result_df[&#34;VIF&#34;].max() &gt;= 10:
        # print(&#39;-&#39;*50)
        # print(&#39;ëº€ ë³€ìˆ˜ :&#39;,result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;][result_df[&#39;VIF&#39;].idxmax()])
        # print(&#39;-&#39;*50)
        return my_auto_linear_regrassion(df.drop(result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;][result_df[&#39;VIF&#39;].idxmax()],axis=1), yname, cv, degree,plot,report,resid_test, figsize, dpi, order,p_value_num )
    else:
        if result_df[&#34;ìœ ì˜í™•ë¥ &#34;].max() &gt;= p_value_num:
            # print(&#39;-&#39;*50)
            # print(&#39;ëº€ ë³€ìˆ˜ :&#39;,result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;][result_df[&#39;ìœ ì˜í™•ë¥ &#39;].idxmax()])
            # print(&#39;-&#39;*50)
            return my_auto_linear_regrassion(df.drop(result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;][result_df[&#39;ìœ ì˜í™•ë¥ &#39;].idxmax()],axis=1), yname,cv, degree,plot,report,resid_test, figsize, dpi, order,p_value_num )
    
    x_train, x_test, y_train, y_test = my_train_test_split(df, yname, test_size=0.2)
    
    xnames = x_train.columns
    yname = y_train.name
    size = len(xnames)

    # ë¶„ì„ëª¨ë¸ ìƒì„±
    model = LinearRegression(n_jobs=-1) # n_jobs : ì‚¬ìš©í•˜ëŠ” cpu ì½”ì–´ì˜ ê°œìˆ˜ // -1ì€ ìµœëŒ€ì¹˜

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv &gt; 0:
        params = {}
        grid = GridSearchCV(model, param_grid=params, cv=cv, n_jobs=-1)
        fit = grid.fit(x_train, y_train)
        model = fit.best_estimator_
        fit.best_params = fit.best_params_
        
        result_df = DataFrame(grid.cv_results_[&#39;params&#39;])
        result_df[&#39;mean_test_score&#39;] = grid.cv_results_[&#39;mean_test_score&#39;]
        
        print(&#34;[êµì°¨ê²€ì¦]&#34;)
        my_pretty_table(result_df.sort_values(by=&#39;mean_test_score&#39;, ascending=False))
        print(&#34;&#34;)

    fit = model.fit(x_train, y_train)
    x = x_test
    y = y_test
    y_pred = fit.predict(x)
    expr = &#34;{yname} = &#34;.format(yname=yname)

    for i, v in enumerate(xnames):
        expr += &#34;%0.3f * %s + &#34; % (fit.coef_[i], v)

    expr += &#34;%0.3f&#34; % fit.intercept_
    print(&#34;[íšŒê·€ì‹]&#34;)
    print(expr, end=&#34;\n\n&#34;)
    resid = y - y_pred

    # ì ˆí¸ê³¼ ê³„ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
    params = np.append(fit.intercept_, fit.coef_)

    # ê²€ì¦ìš© ë…ë¦½ë³€ìˆ˜ì— ìƒìˆ˜í•­ ì¶”ê°€
    design_x = x.copy()
    design_x.insert(0, &#39;ìƒìˆ˜&#39;, 1)

    dot = np.dot(design_x.T,design_x)   # í–‰ë ¬ê³±
    inv = np.linalg.inv(dot)            # ì—­í–‰ë ¬
    dia = inv.diagonal()                # ëŒ€ê°ì›ì†Œ

    # ì œê³±ì˜¤ì°¨
    MSE = (sum((y-y_pred)**2)) / (len(design_x)-len(design_x.iloc[0]))

    se_b = np.sqrt(MSE * dia)           # í‘œì¤€ì˜¤ì°¨
    ts_b = params / se_b                # tê°’

    # ê° ë…ë¦½ìˆ˜ì— ëŒ€í•œ pvalue
    p_values = [2*(1-t.cdf(np.abs(i),(len(design_x)-len(design_x.iloc[0])))) for i in ts_b]

    # VIF
    if len(x.columns) &gt; 1:
        vif = [variance_inflation_factor(x, list(x.columns).index(v)) for i, v in enumerate(x.columns)]
    else:
        vif = 0

    # í‘œì¤€í™” ê³„ìˆ˜
    train_df = x.copy()
    train_df[y.name] = y
    scaler = StandardScaler()
    std = scaler.fit_transform(train_df)
    std_df = DataFrame(std, columns=train_df.columns)
    std_x = std_df[xnames]
    std_y = std_df[yname]
    std_model = LinearRegression()
    std_fit = std_model.fit(std_x, std_y)
    beta = std_fit.coef_

    # ê²°ê³¼í‘œ êµ¬ì„±í•˜ê¸°
    result_df = DataFrame({
        &#34;ì¢…ì†ë³€ìˆ˜&#34;: [yname] * len(xnames),
        &#34;ë…ë¦½ë³€ìˆ˜&#34;: xnames,
        &#34;B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)&#34;: np.round(params[1:], 4),
        &#34;í‘œì¤€ì˜¤ì°¨&#34;: np.round(se_b[1:], 3),
        &#34;Î²(í‘œì¤€í™” ê³„ìˆ˜)&#34;: np.round(beta, 3),
        &#34;t&#34;: np.round(ts_b[1:], 3),
        &#34;ìœ ì˜í™•ë¥ &#34;: np.round(p_values[1:], 3),
        &#34;VIF&#34;: vif,
    })
    if order:
        order = order.upper()
        if order == &#39;V&#39;:
            result_df.sort_values(&#39;VIF&#39;,inplace=True)
        elif  order == &#39;P&#39;:
            result_df.sort_values(&#39;ìœ ì˜í™•ë¥ &#39;,inplace=True)
        # result_df
    my_pretty_table(result_df)
        
    resid = y - y_pred        # ì”ì°¨
    dw = durbin_watson(resid)               # ë”ë¹ˆ ì™“ìŠ¨ í†µê³„ëŸ‰
    r2 = r2_score(y, y_pred)  # ê²°ì •ê³„ìˆ˜(ì„¤ëª…ë ¥)
    rowcount = len(x)                # í‘œë³¸ìˆ˜
    featurecount = len(x.columns)    # ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜

    # ë³´ì •ëœ ê²°ì •ê³„ìˆ˜
    adj_r2 = 1 - (1 - r2) * (rowcount-1) / (rowcount-featurecount-1)

    # fê°’
    f_statistic = (r2 / featurecount) / ((1 - r2) / (rowcount - featurecount - 1))

    # Prob (F-statistic)
    p = 1 - f.cdf(f_statistic, featurecount, rowcount - featurecount - 1)

    tpl = f&#34;ğ‘…^2({r2:.3f}), Adj.ğ‘…^2({adj_r2:.3f}), F({f_statistic:.3f}), P-value({p:.4g}), Durbin-Watson({dw:.3f})&#34;
    print(tpl, end=&#34;\n\n&#34;)

    # ê²°ê³¼ë³´ê³ 
    tpl = f&#34;{yname}ì— ëŒ€í•˜ì—¬ {&#39;,&#39;.join(xnames)}ë¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼, ì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜{&#39;í•˜ë‹¤&#39; if p &lt;= 0.05 else &#39;í•˜ì§€ ì•Šë‹¤&#39;}(F({len(x.columns)},{len(x.index)-len(x.columns)-1}) = {f_statistic:0.3f}, p {&#39;&lt;=&#39; if p &lt;= p_value_num else &#39;&gt;&#39;} 0.05).&#34;

    print(tpl, end = &#39;\n\n&#39;)

    # ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
    for n in xnames:
        item = result_df[result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;] == n]
        coef = item[&#39;B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)&#39;].values[0]
        pvalue = item[&#39;ìœ ì˜í™•ë¥ &#39;].values[0]

        s = f&#34;{n}ì˜ íšŒê·€ê³„ìˆ˜ëŠ” {coef:0.3f}(p {&#39;&lt;=&#39; if pvalue &lt;= p_value_num else &#39;&gt;&#39;} 0.05)ë¡œ, {yname}ì— ëŒ€í•˜ì—¬ {&#39;ìœ ì˜ë¯¸í•œ&#39; if pvalue &lt;= p_value_num else &#39;ìœ ì˜í•˜ì§€ ì•Šì€&#39;} ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.&#34;

        print(s)
        
    print(&#34;&#34;)
    return fit</code></pre>
</details>
</dd>
<dt id="helper.regrassion.my_lasso_regrassion"><code class="name flex">
<span>def <span class="ident">my_lasso_regrassion</span></span>(<span>x_train:Â pandas.core.frame.DataFrame, y_train:Â pandas.core.series.Series, x_test:Â pandas.core.frame.DataFrameÂ =Â None, y_test:Â pandas.core.series.SeriesÂ =Â None, cv:Â intÂ =Â 5, learning_curve:Â boolÂ =Â True, report=False, plot:Â boolÂ =Â False, degree:Â intÂ =Â 1, resid_test=False, figsize=(10, 5), dpi:Â intÂ =Â 100, sort:Â strÂ =Â None, params:Â dictÂ =Â {'alpha': [0.01, 0.1, 1, 10, 100]}) â€‘>Â sklearn.linear_model._base.LinearRegression</span>
</code></dt>
<dd>
<div class="desc"><p>ë¼ì˜íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x_train</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°</dd>
<dt><strong><code>y_train</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°</dd>
<dt><strong><code>x_test</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.</dd>
<dt><strong><code>y_test</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.</dd>
<dt><strong><code>cv</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 5.</dd>
<dt><strong><code>learning_curve</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.</dd>
<dt><strong><code>report</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ì‹œê°í™” ì—¬ë¶€. Defaults to True.</dd>
<dt><strong><code>degree</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.</dd>
<dt><strong><code>resid_test</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.</dd>
<dt><strong><code>sort</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)</dd>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to {'alpha': [0.01, 0.1, 1, 10, 100]}.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Lasso</code></dt>
<dd>Lasso ëª¨ë¸</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def my_lasso_regrassion(x_train: DataFrame, y_train: Series, x_test: DataFrame = None, y_test: Series = None, cv: int = 5, learning_curve: bool = True, report=False, plot: bool = False, degree: int = 1, resid_test=False, figsize=(10, 5), dpi: int = 100, sort: str = None, params: dict = {&#39;alpha&#39;: [0.01, 0.1, 1, 10, 100]}) -&gt; LinearRegression:
    &#34;&#34;&#34;ë¼ì˜íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 5.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to {&#39;alpha&#39;: [0.01, 0.1, 1, 10, 100]}.
        
    Returns:
        Lasso: Lasso ëª¨ë¸
    &#34;&#34;&#34;
    
    #------------------------------------------------------
    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv &gt; 0:   
        # ë¶„ì„ëª¨ë¸ ìƒì„±
        prototype_estimator = Lasso()     
        
        print(&#34;[%s í•˜ì´í¼íŒŒë¼ë¯¸í„°]&#34; % prototype_estimator.__class__.__name__)
        my_pretty_table(DataFrame(params))
        print(&#34;&#34;)
        
        grid = GridSearchCV(prototype_estimator, param_grid=params, cv=cv, n_jobs=-1)
        grid.fit(x_train, y_train)
        
        result_df = DataFrame(grid.cv_results_[&#39;params&#39;])
        result_df[&#39;mean_test_score&#39;] = grid.cv_results_[&#39;mean_test_score&#39;]
        
        print(&#34;[êµì°¨ê²€ì¦]&#34;)
        my_pretty_table(result_df.sort_values(by=&#39;mean_test_score&#39;, ascending=False))
        print(&#34;&#34;)
        
        estimator = grid.best_estimator_
        estimator.best_params = grid.best_params_
    else:
        # ë¶„ì„ëª¨ë¸ ìƒì„±
        estimator = Lasso(**params) 
        estimator.fit(x_train, y_train)
    
    #------------------------------------------------------
    xnames = x_train.columns
    yname = y_train.name
    
    # í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ì¶”ì •ì¹˜ ìƒì„±
    y_pred = estimator.predict(x_test) if x_test is not None else estimator.predict(x_train)
    
    # ë„ì¶œëœ ê²°ê³¼ë¥¼ íšŒê·€ëª¨ë¸ ê°ì²´ì— í¬í•¨ì‹œí‚´
    estimator.x = x_test if x_test is not None else x_train
    estimator.y = y_test if y_test is not None else y_train
    estimator.y_pred = y_pred if y_test is not None else estimator.predict(x_train)
    estimator.resid = y_test - y_pred if y_test is not None else y_train - estimator.predict(x_train)

    #------------------------------------------------------
    # ì„±ëŠ¥í‰ê°€
    if x_test is not None and y_test is not None:
        my_regrassion_result(estimator, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, learning_curve=learning_curve, cv=cv, figsize=figsize, dpi=dpi)
    else:
        my_regrassion_result(estimator, x_train=x_train, y_train=y_train, learning_curve=learning_curve, cv=cv, figsize=figsize, dpi=dpi)

    #------------------------------------------------------
    # ë³´ê³ ì„œ ì¶œë ¥
    if report:
        print(&#34;&#34;)
        my_regrassion_report(estimator, estimator.x, estimator.y, sort, plot=plot, degree=degree, figsize=figsize, dpi=dpi)
    
    #------------------------------------------------------
    # ì”ì°¨ ê°€ì • í™•ì¸  
    if resid_test:
        print(&#34;\n\n[ì”ì°¨ì˜ ê°€ì • í™•ì¸] ==============================&#34;)
        my_resid_test(estimator.x, estimator.y, estimator.y_pred, figsize=figsize, dpi=dpi)

    return estimator</code></pre>
</details>
</dd>
<dt id="helper.regrassion.my_linear_regrassion"><code class="name flex">
<span>def <span class="ident">my_linear_regrassion</span></span>(<span>x_train:Â pandas.core.frame.DataFrame, y_train:Â pandas.core.series.Series, x_test:Â pandas.core.frame.DataFrameÂ =Â None, y_test:Â pandas.core.series.SeriesÂ =Â None, cv:Â intÂ =Â 5, learning_curve:Â boolÂ =Â True, degree:Â intÂ =Â 1, plot:Â boolÂ =Â True, report=True, resid_test=False, figsize=(10, 4), dpi=150, sort:Â strÂ =Â None, order:Â strÂ =Â None, p_value_num:Â floatÂ =Â 0.05) â€‘>Â sklearn.linear_model._base.LinearRegression</span>
</code></dt>
<dd>
<div class="desc"><p>ì„ í˜•íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x_train</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°</dd>
<dt><strong><code>y_train</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°</dd>
<dt><strong><code>x_test</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.</dd>
<dt><strong><code>y_test</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.</dd>
<dt><strong><code>cv</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 5.</dd>
<dt><strong><code>learning_curve</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.</dd>
<dt><strong><code>degree</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ì‹œê°í™” ì—¬ë¶€. Defaults to True.</dd>
<dt><strong><code>report</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.</dd>
<dt><strong><code>resid_test</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 4).</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 150.</dd>
<dt><strong><code>order</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)</dd>
</dl>
<p>p_value_num (float, optional) : íšŒê·€ëª¨í˜•ì˜ ìœ ì˜í™•ë¥ . Drfaults to 0.05</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>LinearRegression</code></dt>
<dd>íšŒê·€ë¶„ì„ ëª¨ë¸</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def my_linear_regrassion(x_train: DataFrame, y_train: Series, x_test: DataFrame = None, y_test: Series = None, cv: int = 5,  learning_curve: bool = True, degree : int = 1, plot: bool = True, report=True, resid_test=False, figsize=(10, 4), dpi=150, sort: str = None,order: str = None,p_value_num:float=0.05 ) -&gt; LinearRegression:
    &#34;&#34;&#34;ì„ í˜•íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 5.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 4).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 150.
        order (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        p_value_num (float, optional) : íšŒê·€ëª¨í˜•ì˜ ìœ ì˜í™•ë¥ . Drfaults to 0.05
    Returns:
        LinearRegression: íšŒê·€ë¶„ì„ ëª¨ë¸
    &#34;&#34;&#34;
    xnames = x_train.columns
    yname = y_train.name
    size = len(xnames)

    # ë¶„ì„ëª¨ë¸ ìƒì„±

    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv &gt; 0:
        params = {}
        prototype_estimator = LinearRegression(n_jobs=-1)
        grid = GridSearchCV(prototype_estimator, param_grid=params, cv=cv, n_jobs=-1)
        grid.fit(x_train, y_train)
        result_df = DataFrame(grid.cv_results_[&#39;params&#39;])
        result_df[&#39;mean_test_score&#39;] = grid.cv_results_[&#39;mean_test_score&#39;]
        print(&#34;[êµì°¨ê²€ì¦]&#34;)
        my_pretty_table(result_df.sort_values(by=&#39;mean_test_score&#39;, ascending=False))
        print(&#34;&#34;)

        estimator = grid.best_estimator_
        estimator.best_params = grid.best_params_
    else:
        estimator = LinearRegression(n_jobs=-1)
        estimator.fit(x_train, y_train)        
    y_pred = estimator.predict(x_test) if x_test is not None else estimator.predict(x_train)
    
    # ë„ì¶œëœ ê²°ê³¼ë¥¼ íšŒê·€ëª¨ë¸ ê°ì²´ì— í¬í•¨ì‹œí‚´
    estimator.x = x_test if x_test is not None else x_train
    estimator.y = y_test if y_test is not None else y_train
    estimator.y_pred = y_pred if y_test is not None else estimator.predict(x_train)
    estimator.resid = y_test - y_pred if y_test is not None else y_train - estimator.predict(x_train)

    #------------------------------------------------------
    # ì„±ëŠ¥í‰ê°€
    if x_test is not None and y_test is not None:
        my_regrassion_result(estimator, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, learning_curve=learning_curve, cv=cv, figsize=figsize, dpi=dpi)
    else:
        my_regrassion_result(estimator, x_train=x_train, y_train=y_train, learning_curve=learning_curve, cv=cv, figsize=figsize, dpi=dpi)

    #------------------------------------------------------
    # ë³´ê³ ì„œ ì¶œë ¥
    if report:
        print(&#34;&#34;)
        my_regrassion_report(estimator, estimator.x, estimator.y, sort=sort, plot=plot, degree=degree, figsize=figsize, dpi=dpi)
    
    #------------------------------------------------------
    # ì”ì°¨ ê°€ì • í™•ì¸  
    if resid_test:
        print(&#34;\n\n[ì”ì°¨ì˜ ê°€ì • í™•ì¸] ==============================&#34;)
        my_resid_test(estimator.x, estimator.y, estimator.y_pred, figsize=figsize, dpi=dpi)

    return estimator</code></pre>
</details>
</dd>
<dt id="helper.regrassion.my_regrassion_report"><code class="name flex">
<span>def <span class="ident">my_regrassion_report</span></span>(<span>estimator:Â <built-inÂ functionÂ any>, x:Â pandas.core.frame.DataFrameÂ =Â None, y:Â pandas.core.series.SeriesÂ =Â None, sort:Â strÂ =Â None, plot:Â boolÂ =Â False, degree:Â intÂ =Â 1, figsize:Â tupleÂ =Â (10, 5), dpi:Â intÂ =Â 100, order:Â strÂ =Â None, p_value_num:Â floatÂ =Â 0.05) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>ì„ í˜•íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ í•œë‹¤.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>fit</code></strong> :&ensp;<code>LinearRegression</code></dt>
<dd>ì„ í˜•íšŒê·€ ê°ì²´</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°</dd>
<dt><strong><code>sort</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>ì •ë ¬ ê¸°ì¤€ (v, p). Defaults to None.</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ì‹œê°í™” ì—¬ë¶€. Defaults to False.</dd>
<dt><strong><code>degree</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.</dd>
<dt><strong><code>order</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)</dd>
</dl>
<p>p_value_num (float, optional) : íšŒê·€ëª¨í˜•ì˜ ìœ ì˜í™•ë¥ . Drfaults to 0.05</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def my_regrassion_report(estimator: any, x: DataFrame = None, y: Series = None, sort: str = None, plot: bool = False, degree: int = 1, figsize: tuple = (10, 5), dpi: int = 100, order : str = None, p_value_num:float=0.05 ) -&gt; None:
    &#34;&#34;&#34;ì„ í˜•íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ í•œë‹¤.

    Args:
        fit (LinearRegression): ì„ í˜•íšŒê·€ ê°ì²´
        x (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        sort (str, optional): ì •ë ¬ ê¸°ì¤€ (v, p). Defaults to None.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to False.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        order (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        p_value_num (float, optional) : íšŒê·€ëª¨í˜•ì˜ ìœ ì˜í™•ë¥ . Drfaults to 0.05
    &#34;&#34;&#34;
    
    # íšŒê·€ì‹
    xnames = x.columns
    yname = y.name
    
    expr = &#34;{yname} = &#34;.format(yname=yname)

    for i, v in enumerate(xnames):
        expr += &#34;%0.3f * %s + &#34; % (estimator.coef_[i], v)

    expr += &#34;%0.3f&#34; % estimator.intercept_
    print(&#34;[íšŒê·€ì‹]&#34;)
    print(expr, end=&#34;\n\n&#34;)
    
    
    print(&#34;[ë…ë¦½ë³€ìˆ˜ë³´ê³ ]&#34;)
    if x is None and y is None:
        x = estimator.x
        y = estimator.y
    
    y_pred = estimator.predict(x)
    xnames = x.columns
    yname = y.name

    # ì”ì°¨
    resid = y - y_pred

    # ì ˆí¸ê³¼ ê³„ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
    params = np.append(estimator.intercept_, estimator.coef_)

    # ê²€ì¦ìš© ë…ë¦½ë³€ìˆ˜ì— ìƒìˆ˜í•­ ì¶”ê°€
    design_x = x.copy()
    design_x.insert(0, &#39;ìƒìˆ˜&#39;, 1)

    dot = np.dot(design_x.T,design_x)   # í–‰ë ¬ê³±
    inv = np.linalg.inv(dot)            # ì—­í–‰ë ¬
    dia = inv.diagonal()                # ëŒ€ê°ì›ì†Œ

    # ì œê³±ì˜¤ì°¨
    MSE = (sum((y-y_pred)**2)) / (len(design_x)-len(design_x.iloc[0]))

    se_b = np.sqrt(MSE * dia)           # í‘œì¤€ì˜¤ì°¨
    ts_b = params / se_b                # tê°’

    # ê° ë…ë¦½ìˆ˜ì— ëŒ€í•œ pvalue
    p_values = [2*(1-t.cdf(np.abs(i),(len(design_x)-len(design_x.iloc[0])))) for i in ts_b]

    # VIF
    if len(x.columns) &gt; 1:
        vif = [variance_inflation_factor(x, list(x.columns).index(v)) for i, v in enumerate(x.columns)]
    else:
        vif = 0

    # í‘œì¤€í™” ê³„ìˆ˜
    train_df = x.copy()
    train_df[y.name] = y
    scaler = StandardScaler()
    std = scaler.fit_transform(train_df)
    std_df = DataFrame(std, columns=train_df.columns)
    std_x = std_df[xnames]
    std_y = std_df[yname]
    std_estimator = LinearRegression(n_jobs=-1)
    std_estimator.fit(std_x, std_y)
    beta = std_estimator.coef_

    # ê²°ê³¼í‘œ êµ¬ì„±í•˜ê¸°
    result_df = DataFrame({
        &#34;ì¢…ì†ë³€ìˆ˜&#34;: [yname] * len(xnames),
        &#34;ë…ë¦½ë³€ìˆ˜&#34;: xnames,
        &#34;B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)&#34;: np.round(params[1:], 4),
        &#34;í‘œì¤€ì˜¤ì°¨&#34;: np.round(se_b[1:], 3),
        &#34;Î²(í‘œì¤€í™” ê³„ìˆ˜)&#34;: np.round(beta, 3),
        &#34;t&#34;: np.round(ts_b[1:], 3),
        &#34;ìœ ì˜í™•ë¥ &#34;: np.round(p_values[1:], 3),
        &#34;VIF&#34;: vif,
    })
    
    if sort:
        if sort.upper() == &#39;V&#39;:
            result_df.sort_values(&#39;VIF&#39;, inplace=True)
        elif sort.upper() == &#39;P&#39;:
            result_df.sort_values(&#39;ìœ ì˜í™•ë¥ &#39;, inplace=True)
    

    #result_df
    my_pretty_table(result_df)
    print(&#34;&#34;)

    resid = y - y_pred        # ì”ì°¨
    dw = durbin_watson(resid)               # ë”ë¹ˆ ì™“ìŠ¨ í†µê³„ëŸ‰
    r2 = r2_score(y, y_pred)  # ê²°ì •ê³„ìˆ˜(ì„¤ëª…ë ¥)
    rowcount = len(x)                # í‘œë³¸ìˆ˜
    featurecount = len(x.columns)    # ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜

    # ë³´ì •ëœ ê²°ì •ê³„ìˆ˜
    adj_r2 = 1 - (1 - r2) * (rowcount-1) / (rowcount-featurecount-1)

    # fê°’
    f_statistic = (r2 / featurecount) / ((1 - r2) / (rowcount - featurecount - 1))

    # Prob (F-statistic)
    p = 1 - f.cdf(f_statistic, featurecount, rowcount - featurecount - 1)

    tpl = &#34;ğ‘…^2(%.3f), Adj.ğ‘…^2(%.3f), F(%.3f), P-value(%.4g), Durbin-Watson(%.3f)&#34;
    print(tpl % (r2, adj_r2, f_statistic, p, dw), end=&#34;\n\n&#34;)

    # ê²°ê³¼ë³´ê³ 
    tpl = &#34;%sì— ëŒ€í•˜ì—¬ %së¡œ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¶„ì„ì„ ì‹¤ì‹œí•œ ê²°ê³¼,\nì´ íšŒê·€ëª¨í˜•ì€ í†µê³„ì ìœ¼ë¡œ %s(F(%s,%s) = %0.3f, p %s %s).&#34;

    result_str = tpl % (
        yname,
        &#34;,&#34;.join(xnames),
        &#34;ìœ ì˜í•˜ë‹¤&#34; if p &lt;= p_value_num else &#34;ìœ ì˜í•˜ì§€ ì•Šë‹¤&#34;,
        len(x.columns),
        len(x.index)-len(x.columns)-1,
        f_statistic,
        &#34;&lt;=&#34; if p &lt;= p_value_num else &#34;&gt;&#34;,
        p_value_num)
        
    print(result_str, end=&#34;\n\n&#34;)

    # ë…ë¦½ë³€ìˆ˜ ë³´ê³ 
    for n in xnames:
        item = result_df[result_df[&#39;ë…ë¦½ë³€ìˆ˜&#39;] == n]
        coef = item[&#39;B(ë¹„í‘œì¤€í™” ê³„ìˆ˜)&#39;].values[0]
        pvalue = item[&#39;ìœ ì˜í™•ë¥ &#39;].values[0]

        s = &#34;%sì˜ íšŒê·€ê³„ìˆ˜ëŠ” %0.3f(p %s %s)ë¡œ, %sì— ëŒ€í•˜ì—¬ %s.&#34;
        k = s % (n,
                coef,
                &#34;&lt;=&#34; if pvalue &lt;= p_value_num else &#39;&gt;&#39;,
                yname,
                &#39;ìœ ì˜ë¯¸í•œ ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤&#39; if pvalue &lt;= p_value_num else &#39;ìœ ì˜í•˜ì§€ ì•Šì€ ì˜ˆì¸¡ë³€ì¸ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤&#39;,
                p_value_num
        )

        print(k)
        
    # ë„ì¶œëœ ê²°ê³¼ë¥¼ íšŒê·€ëª¨ë¸ ê°ì²´ì— í¬í•¨ì‹œí‚´ --&gt; ê°ì²´ íƒ€ì…ì˜ íŒŒë¼ë¯¸í„°ëŠ” ì°¸ì¡°ë³€ìˆ˜ë¡œ ì „ë‹¬ë˜ë¯€ë¡œ fit ê°ì²´ì— í¬í•¨ëœ ê²°ê³¼ê°’ë“¤ì€ ì´ í•¨ìˆ˜ ì™¸ë¶€ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.
    estimator.r2 = r2
    estimator.adj_r2 = adj_r2
    estimator.f_statistic = f_statistic
    estimator.p = p
    estimator.dw = dw
        
    # ì‹œê°í™”
    if plot:
        for i, v in enumerate(xnames):
            plt.figure(figsize=figsize, dpi=dpi)
            
            if degree == 1:
                sb.regplot(x=x[v], y=y, ci=95, label=&#39;ê´€ì¸¡ì¹˜&#39;)
                sb.regplot(x=x[v], y=y_pred, ci=0, label=&#39;ì¶”ì •ì¹˜&#39;)
            else:
                sb.scatterplot(x=x[v], y=y, label=&#39;ê´€ì¸¡ì¹˜&#39;)
                sb.scatterplot(x=x[v], y=y_pred, label=&#39;ì¶”ì •ì¹˜&#39;)
                
                t1 = my_trend(x[v], y, degree=degree)
                sb.lineplot(x=t1[0], y=t1[1], color=&#39;blue&#39;, linestyle=&#39;--&#39;, label=&#39;ê´€ì¸¡ì¹˜ ì¶”ì„¸ì„ &#39;)
                
                t2 = my_trend(x[v], y_pred, degree=degree)
                sb.lineplot(x=t2[0], y=t2[1], color=&#39;red&#39;, linestyle=&#39;--&#39;, label=&#39;ì¶”ì •ì¹˜ ì¶”ì„¸ì„ &#39;)
            
            plt.title(f&#34;{yname} vs {v}&#34;)
            plt.legend()
            plt.grid()

            plt.show()
            plt.close()</code></pre>
</details>
</dd>
<dt id="helper.regrassion.my_regrassion_result"><code class="name flex">
<span>def <span class="ident">my_regrassion_result</span></span>(<span>estimator:Â <built-inÂ functionÂ any>, x_train:Â pandas.core.frame.DataFrameÂ =Â None, y_train:Â pandas.core.series.SeriesÂ =Â None, x_test:Â pandas.core.frame.DataFrameÂ =Â None, y_test:Â pandas.core.series.SeriesÂ =Â None, learning_curve:Â boolÂ =Â True, cv:Â intÂ =Â 10, figsize:Â tupleÂ =Â (10, 5), dpi:Â intÂ =Â 100) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>estimator</code></strong> :&ensp;<code>any</code></dt>
<dd>íšŒê·€ë¶„ì„ ëª¨ë¸</dd>
<dt><strong><code>x_train</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°</dd>
<dt><strong><code>y_train</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°</dd>
<dt><strong><code>x_test</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.</dd>
<dt><strong><code>y_test</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.</dd>
<dt><strong><code>learning_curve</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.</dd>
<dt><strong><code>cv</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 10.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def my_regrassion_result(estimator: any, x_train: DataFrame = None, y_train: Series = None, x_test: DataFrame = None, y_test: Series = None, learning_curve: bool = True, cv: int = 10, figsize: tuple = (10, 5), dpi: int = 100) -&gt; None:
    &#34;&#34;&#34;íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        estimator (any): íšŒê·€ë¶„ì„ ëª¨ë¸
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to False.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 10.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
    &#34;&#34;&#34;
    
    scores = []
    score_names = []
    
    if x_train is not None and y_train is not None:
        y_train_pred = estimator.predict(x_train)

        # ì„±ëŠ¥í‰ê°€
        result = {
            &#34;ê²°ì •ê³„ìˆ˜(R2)&#34;: r2_score(y_train, y_train_pred),
            &#34;í‰ê· ì ˆëŒ€ì˜¤ì°¨(MAE)&#34;: mean_absolute_error(y_train, y_train_pred),
            &#34;í‰ê· ì œê³±ì˜¤ì°¨(MSE)&#34;: mean_squared_error(y_train, y_train_pred),
            &#34;í‰ê· ì˜¤ì°¨(RMSE)&#34;: np.sqrt(mean_squared_error(y_train, y_train_pred)),
            &#34;í‰ê·  ì ˆëŒ€ ë°±ë¶„ì˜¤ì°¨ ë¹„ìœ¨(MAPE)&#34;: np.mean(np.abs((y_train - y_train_pred) / y_train) * 100),
            &#34;í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨(MPE)&#34;: np.mean((y_train - y_train_pred) / y_train * 100)
        }
        
        scores.append(result)
        score_names.append(&#34;í›ˆë ¨ë°ì´í„°&#34;)
        
    if x_test is not None and y_test is not None:
        y_test_pred = estimator.predict(x_test)

        # ì„±ëŠ¥í‰ê°€
        result = {
            &#34;ê²°ì •ê³„ìˆ˜(R2)&#34;: r2_score(y_test, y_test_pred),
            &#34;í‰ê· ì ˆëŒ€ì˜¤ì°¨(MAE)&#34;: mean_absolute_error(y_test, y_test_pred),
            &#34;í‰ê· ì œê³±ì˜¤ì°¨(MSE)&#34;: mean_squared_error(y_test, y_test_pred),
            &#34;í‰ê· ì˜¤ì°¨(RMSE)&#34;: np.sqrt(mean_squared_error(y_test, y_test_pred)),
            &#34;í‰ê·  ì ˆëŒ€ ë°±ë¶„ì˜¤ì°¨ ë¹„ìœ¨(MAPE)&#34;: np.mean(np.abs((y_test - y_test_pred) / y_test) * 100),
            &#34;í‰ê·  ë¹„ìœ¨ ì˜¤ì°¨(MPE)&#34;: np.mean((y_test - y_test_pred) / y_test * 100)
        }
        
        scores.append(result)
        score_names.append(&#34;ê²€ì¦ë°ì´í„°&#34;)
        

    print(&#34;[íšŒê·€ë¶„ì„ ì„±ëŠ¥í‰ê°€]&#34;)
    result_df = DataFrame(scores, index=score_names)
    my_pretty_table(result_df.T)
    
    # í•™ìŠµê³¡ì„ 
    if learning_curve:
        print(&#34;\n[í•™ìŠµê³¡ì„ ]&#34;)
        yname = y_train.name
        
        if x_test is not None and y_test is not None:
            y_df = concat([y_train, y_test])
            x_df = concat([x_train, x_test])
        else:
            y_df = y_train.copy()
            x_df = x_train.copy()
            
        x_df[yname] = y_df 
        x_df.sort_index(inplace=True)
        
        if cv &gt; 0:
            my_learing_curve(estimator, data=x_df, yname=yname, cv=cv, scoring=&#39;RMSE&#39;, figsize=figsize, dpi=dpi)
        else:
            my_learing_curve(estimator, data=x_df, yname=yname, scoring=&#39;RMSE&#39;, figsize=figsize, dpi=dpi)</code></pre>
</details>
</dd>
<dt id="helper.regrassion.my_resid_equal_var"><code class="name flex">
<span>def <span class="ident">my_resid_equal_var</span></span>(<span>x:Â pandas.core.frame.DataFrame, y:Â pandas.core.series.Series, y_pred:Â pandas.core.series.Series, p_value_num:Â floatÂ =Â 0.05) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>ë…ë¦½ë³€ìˆ˜</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì¢…ì†ë³€ìˆ˜</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì˜ˆì¸¡ê°’</dd>
</dl>
<p>p_value_num(float) : ìœ ì˜í™•ë¥ </p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def my_resid_equal_var(x: DataFrame, y: Series, y_pred: Series, p_value_num:float =0.05) -&gt; None:
    &#34;&#34;&#34;ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        x (DataFrame): ë…ë¦½ë³€ìˆ˜
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
        p_value_num(float) : ìœ ì˜í™•ë¥ 
    &#34;&#34;&#34;
    # ë…ë¦½ë³€ìˆ˜ ë°ì´í„° í”„ë ˆì„ ë³µì‚¬
    x_copy = x.copy()
    
    # ìƒìˆ˜í•­ ì¶”ê°€
    x_copy.insert(0, &#34;const&#34;, 1)
    
    # ì”ì°¨ êµ¬í•˜ê¸°
    resid = y - y_pred
    
    # ë“±ë¶„ì‚°ì„± ê²€ì •
    bs_result = het_breuschpagan(resid, x_copy)
    bs_result_df = DataFrame(bs_result, columns=[&#39;values&#39;], index=[&#39;statistic&#39;, &#39;p-value&#39;, &#39;f-value&#39;, &#39;f p-value&#39;])

    print(f&#34;ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± ê°€ì • ì¶©ì¡± ì—¬ë¶€: {bs_result[1] &gt; p_value_num}&#34;)
    my_pretty_table(bs_result_df)</code></pre>
</details>
</dd>
<dt id="helper.regrassion.my_resid_independence"><code class="name flex">
<span>def <span class="ident">my_resid_independence</span></span>(<span>y:Â pandas.core.series.Series, y_pred:Â pandas.core.series.Series) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>ì”ì°¨ì˜ ë…ë¦½ì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì¢…ì†ë³€ìˆ˜</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì˜ˆì¸¡ê°’</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def my_resid_independence(y: Series, y_pred: Series) -&gt; None:
    &#34;&#34;&#34;ì”ì°¨ì˜ ë…ë¦½ì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
    &#34;&#34;&#34;
    dw = durbin_watson(y - y_pred)
    print(f&#34;Durbin-Watson: {dw}, ì”ì°¨ì˜ ë…ë¦½ì„± ê°€ì • ë§Œì¡± ì—¬ë¶€: {1.5 &lt; dw &lt; 2.5}&#34;)</code></pre>
</details>
</dd>
<dt id="helper.regrassion.my_resid_normality"><code class="name flex">
<span>def <span class="ident">my_resid_normality</span></span>(<span>y:Â pandas.core.series.Series, y_pred:Â pandas.core.series.Series) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>MSEê°’ì„ ì´ìš©í•˜ì—¬ ì”ì°¨ì˜ ì •ê·œì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì¢…ì†ë³€ìˆ˜</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì˜ˆì¸¡ê°’</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def my_resid_normality(y: Series, y_pred: Series) -&gt; None:
    &#34;&#34;&#34;MSEê°’ì„ ì´ìš©í•˜ì—¬ ì”ì°¨ì˜ ì •ê·œì„± ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
    &#34;&#34;&#34;
    mse = mean_squared_error(y, y_pred)
    resid = y - y_pred
    mse_sq = np.sqrt(mse)

    r1 = resid[ (resid &gt; -mse_sq) &amp; (resid &lt; mse_sq)].count() / resid.count() * 100
    r2 = resid[ (resid &gt; -2*mse_sq) &amp; (resid &lt; 2*mse_sq)].count() / resid.count() * 100
    r3 = resid[ (resid &gt; -3*mse_sq) &amp; (resid &lt; 3*mse_sq)].count() / resid.count() * 100

    mse_r = [r1, r2, r3]
    
    print(f&#34;ë£¨íŠ¸ 1MSE êµ¬ê°„ì— í¬í•¨ëœ ì”ì°¨ ë¹„ìœ¨: {r1:1.2f}% ({r1-68})&#34;)
    print(f&#34;ë£¨íŠ¸ 2MSE êµ¬ê°„ì— í¬í•¨ëœ ì”ì°¨ ë¹„ìœ¨: {r2:1.2f}% ({r2-95})&#34;)
    print(f&#34;ë£¨íŠ¸ 3MSE êµ¬ê°„ì— í¬í•¨ëœ ì”ì°¨ ë¹„ìœ¨: {r3:1.2f}% ({r3-99})&#34;)
    
    normality = r1 &gt;= 68 and r2 &gt;= 95 and r3 &gt;= 99
    print(f&#34;ì”ì°¨ì˜ ì •ê·œì„± ê°€ì • ì¶©ì¡± ì—¬ë¶€: {normality}&#34;)</code></pre>
</details>
</dd>
<dt id="helper.regrassion.my_resid_test"><code class="name flex">
<span>def <span class="ident">my_resid_test</span></span>(<span>x:Â pandas.core.frame.DataFrame, y:Â pandas.core.series.Series, y_pred:Â pandas.core.series.Series, figsize:Â tupleÂ =Â (10, 4), dpi:Â intÂ =Â 150, p_value_num:Â floatÂ =Â 0.05) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í•œë‹¤.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>Series</code></dt>
<dd>ë…ë¦½ë³€ìˆ˜</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì¢…ì†ë³€ìˆ˜</dd>
<dt><strong><code>y_pred</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì˜ˆì¸¡ê°’</dd>
</dl>
<p>p_value_num(float) : ìœ ì˜í™•ë¥ </p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def my_resid_test(x: DataFrame, y: Series, y_pred: Series, figsize: tuple=(10, 4), dpi: int=150, p_value_num:float = 0.05) -&gt; None:
    &#34;&#34;&#34;ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í•œë‹¤.

    Args:
        x (Series): ë…ë¦½ë³€ìˆ˜
        y (Series): ì¢…ì†ë³€ìˆ˜
        y_pred (Series): ì˜ˆì¸¡ê°’
        p_value_num(float) : ìœ ì˜í™•ë¥ 
    &#34;&#34;&#34;

    # ì”ì°¨ ìƒì„±
    resid = y - y_pred
    
    print(&#34;[ì”ì°¨ì˜ ì„ í˜•ì„± ê°€ì •]&#34;)
    my_residplot(y, y_pred, lowess=True, figsize=figsize, dpi=dpi)
    
    print(&#34;\n[ì”ì°¨ì˜ ì •ê·œì„± ê°€ì •]&#34;)
    my_qqplot(y, figsize=figsize, dpi=dpi)
    my_residplot(y, y_pred, mse=True, figsize=figsize, dpi=dpi)
    my_resid_normality(y, y_pred)
    
    print(&#34;\n[ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± ê°€ì •]&#34;)
    my_resid_equal_var(x, y, y_pred, p_value_num)
    
    print(&#34;\n[ì”ì°¨ì˜ ë…ë¦½ì„± ê°€ì •]&#34;)
    my_resid_independence(y, y_pred)</code></pre>
</details>
</dd>
<dt id="helper.regrassion.my_ridge_regrassion"><code class="name flex">
<span>def <span class="ident">my_ridge_regrassion</span></span>(<span>x_train:Â pandas.core.frame.DataFrame, y_train:Â pandas.core.series.Series, x_test:Â pandas.core.frame.DataFrameÂ =Â None, y_test:Â pandas.core.series.SeriesÂ =Â None, cv:Â intÂ =Â 5, learning_curve:Â boolÂ =Â True, report=False, plot:Â boolÂ =Â False, degree:Â intÂ =Â 1, resid_test=False, figsize=(10, 5), dpi:Â intÂ =Â 100, sort:Â strÂ =Â None, params:Â dictÂ =Â {'alpha': [0.01, 0.1, 1, 10, 100]}) â€‘>Â sklearn.linear_model._base.LinearRegression</span>
</code></dt>
<dd>
<div class="desc"><p>ë¦¿ì§€íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x_train</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°</dd>
<dt><strong><code>y_train</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°</dd>
<dt><strong><code>x_test</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.</dd>
<dt><strong><code>y_test</code></strong> :&ensp;<code>Series</code></dt>
<dd>ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.</dd>
<dt><strong><code>cv</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 5.</dd>
<dt><strong><code>learning_curve</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.</dd>
<dt><strong><code>report</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.</dd>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ì‹œê°í™” ì—¬ë¶€. Defaults to True.</dd>
<dt><strong><code>degree</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.</dd>
<dt><strong><code>resid_test</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).</dd>
<dt><strong><code>dpi</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.</dd>
<dt><strong><code>sort</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)</dd>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to {'alpha': [0.01, 0.1, 1, 10, 100]}.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Ridge</code></dt>
<dd>Ridge ëª¨ë¸</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def my_ridge_regrassion(x_train: DataFrame, y_train: Series, x_test: DataFrame = None, y_test: Series = None, cv: int = 5, learning_curve: bool = True, report=False, plot: bool = False, degree: int = 1, resid_test=False, figsize=(10, 5), dpi: int = 100, sort: str = None, params: dict = {&#39;alpha&#39;: [0.01, 0.1, 1, 10, 100]}) -&gt; LinearRegression:
    &#34;&#34;&#34;ë¦¿ì§€íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        x_train (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        y_train (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ í›ˆë ¨ ë°ì´í„°
        x_test (DataFrame): ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        y_test (Series): ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê²€ì¦ ë°ì´í„°. Defaults to None.
        cv (int, optional): êµì°¨ê²€ì¦ íšŸìˆ˜. Defaults to 5.
        learning_curve (bool, optional): í•™ìŠµê³¡ì„ ì„ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        report (bool, optional): íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³ ì„œë¡œ ì¶œë ¥í• ì§€ ì—¬ë¶€. Defaults to True.
        plot (bool, optional): ì‹œê°í™” ì—¬ë¶€. Defaults to True.
        degree (int, optional): ë‹¤í•­íšŒê·€ë¶„ì„ì˜ ì°¨ìˆ˜. Defaults to 1.
        resid_test (bool, optional): ì”ì°¨ì˜ ê°€ì •ì„ í™•ì¸í• ì§€ ì—¬ë¶€. Defaults to False.
        figsize (tuple, optional): ê·¸ë˜í”„ì˜ í¬ê¸°. Defaults to (10, 5).
        dpi (int, optional): ê·¸ë˜í”„ì˜ í•´ìƒë„. Defaults to 100.
        sort (bool, optional): ë…ë¦½ë³€ìˆ˜ ê²°ê³¼ ë³´ê³  í‘œì˜ ì •ë ¬ ê¸°ì¤€ (v, p)
        params (dict, optional): í•˜ì´í¼íŒŒë¼ë¯¸í„°. Defaults to {&#39;alpha&#39;: [0.01, 0.1, 1, 10, 100]}.
        
    Returns:
        Ridge: Ridge ëª¨ë¸
    &#34;&#34;&#34;
    
    #------------------------------------------------------
    # êµì°¨ê²€ì¦ ì„¤ì •
    if cv &gt; 0:   
        # ë¶„ì„ëª¨ë¸ ìƒì„±
        prototype_estimator = Ridge()     
        
        print(&#34;[%s í•˜ì´í¼íŒŒë¼ë¯¸í„°]&#34; % prototype_estimator.__class__.__name__)
        my_pretty_table(DataFrame(params))
        print(&#34;&#34;)
        
        grid = GridSearchCV(prototype_estimator, param_grid=params, cv=cv, n_jobs=-1)
        grid.fit(x_train, y_train)
        
        result_df = DataFrame(grid.cv_results_[&#39;params&#39;])
        result_df[&#39;mean_test_score&#39;] = grid.cv_results_[&#39;mean_test_score&#39;]
        
        print(&#34;[êµì°¨ê²€ì¦]&#34;)
        my_pretty_table(result_df.sort_values(by=&#39;mean_test_score&#39;, ascending=False))
        print(&#34;&#34;)
        
        estimator = grid.best_estimator_
        estimator.best_params = grid.best_params_
    else:
        # ë¶„ì„ëª¨ë¸ ìƒì„±
        estimator = Ridge(**params) 
        estimator.fit(x_train, y_train)
    
    #------------------------------------------------------
    xnames = x_train.columns
    yname = y_train.name
    
    # í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ì¶”ì •ì¹˜ ìƒì„±
    y_pred = estimator.predict(x_test) if x_test is not None else estimator.predict(x_train)
    
    # ë„ì¶œëœ ê²°ê³¼ë¥¼ íšŒê·€ëª¨ë¸ ê°ì²´ì— í¬í•¨ì‹œí‚´
    estimator.x = x_test if x_test is not None else x_train
    estimator.y = y_test if y_test is not None else y_train
    estimator.y_pred = y_pred if y_test is not None else estimator.predict(x_train)
    estimator.resid = y_test - y_pred if y_test is not None else y_train - estimator.predict(x_train)

    #------------------------------------------------------
    # ì„±ëŠ¥í‰ê°€
    if x_test is not None and y_test is not None:
        my_regrassion_result(estimator, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, learning_curve=learning_curve, cv=cv, figsize=figsize, dpi=dpi)
    else:
        my_regrassion_result(estimator, x_train=x_train, y_train=y_train, learning_curve=learning_curve, cv=cv, figsize=figsize, dpi=dpi)

    #------------------------------------------------------
    # ë³´ê³ ì„œ ì¶œë ¥
    if report:
        print(&#34;&#34;)
        my_regrassion_report(estimator, estimator.x, estimator.y, sort, plot=plot, degree=degree, figsize=figsize, dpi=dpi)
    
    #------------------------------------------------------
    # ì”ì°¨ ê°€ì • í™•ì¸  
    if resid_test:
        print(&#34;\n\n[ì”ì°¨ì˜ ê°€ì • í™•ì¸] ==============================&#34;)
        my_resid_test(estimator.x, estimator.y, estimator.y_pred, figsize=figsize, dpi=dpi)

    return estimator</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="helper" href="index.html">helper</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="helper.regrassion.my_auto_linear_regrassion" href="#helper.regrassion.my_auto_linear_regrassion">my_auto_linear_regrassion</a></code></li>
<li><code><a title="helper.regrassion.my_lasso_regrassion" href="#helper.regrassion.my_lasso_regrassion">my_lasso_regrassion</a></code></li>
<li><code><a title="helper.regrassion.my_linear_regrassion" href="#helper.regrassion.my_linear_regrassion">my_linear_regrassion</a></code></li>
<li><code><a title="helper.regrassion.my_regrassion_report" href="#helper.regrassion.my_regrassion_report">my_regrassion_report</a></code></li>
<li><code><a title="helper.regrassion.my_regrassion_result" href="#helper.regrassion.my_regrassion_result">my_regrassion_result</a></code></li>
<li><code><a title="helper.regrassion.my_resid_equal_var" href="#helper.regrassion.my_resid_equal_var">my_resid_equal_var</a></code></li>
<li><code><a title="helper.regrassion.my_resid_independence" href="#helper.regrassion.my_resid_independence">my_resid_independence</a></code></li>
<li><code><a title="helper.regrassion.my_resid_normality" href="#helper.regrassion.my_resid_normality">my_resid_normality</a></code></li>
<li><code><a title="helper.regrassion.my_resid_test" href="#helper.regrassion.my_resid_test">my_resid_test</a></code></li>
<li><code><a title="helper.regrassion.my_ridge_regrassion" href="#helper.regrassion.my_ridge_regrassion">my_ridge_regrassion</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>